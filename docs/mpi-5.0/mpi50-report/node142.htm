<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-coll/coll-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title><font face="sans-serif"> MPI_REDUCE_SCATTER</font></title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node142">7.10.2.  MPI_REDUCE_SCATTER</span></h2>
<a href="node141.htm#Node141"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node140.htm#Node140"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node143.htm#Node143"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node140.htm#Node140"> Reduce-Scatter</a>
<b>Next: </b><a href="node143.htm#Node143"> Scan</a>
<b>Previous: </b><a href="node141.htm#Node141"> <font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font></a>
<p>
  
<font face="sans-serif"> MPI_REDUCE_SCATTER</font> extends the functionality of <font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font>  
such that the scattered blocks can vary in size.  
Block sizes are determined by the <font face="sans-serif"> recvcounts</font> array,  
such that the <font face="sans-serif"> i</font>-th block contains <font face="sans-serif"> recvcounts[i]</font> elements.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_REDUCE_SCATTER(<span style="white-space:nowrap">sendbuf</span>, <span style="white-space:nowrap">recvbuf</span>, <span style="white-space:nowrap">recvcounts</span>, <span style="white-space:nowrap">datatype</span>, <span style="white-space:nowrap">op</span>, <span style="white-space:nowrap">comm</span>)</TD></TR>  
<TR><TD><span style="font-size:0.900em;"> IN</span> sendbuf</TD><TD>starting address of send buffer (choice)</TD></TR>  
<TR><TD><span style="font-size:0.900em;"> OUT</span> recvbuf</TD><TD>starting address of receive buffer (choice)</TD></TR>  
<TR><TD><span style="font-size:0.900em;"> IN</span> recvcounts</TD><TD>nonnegative integer array (of length group size) specifying the number of elements of the result distributed to each <font face="sans-serif"> MPI</font> process.</TD></TR>  
<TR><TD><span style="font-size:0.900em;"> IN</span> datatype</TD><TD>datatype of elements of send and receive buffers (handle)</TD></TR>  
<TR><TD><span style="font-size:0.900em;"> IN</span> op</TD><TD>operation (handle)</TD></TR>  
<TR><TD><span style="font-size:0.900em;"> IN</span> comm</TD><TD>communicator (handle)</TD></TR>  
</TABLE>  
  <b> C binding</b><br>  <tt> int MPI_Reduce_scatter(const void *sendbuf, void *recvbuf, const int recvcounts[], MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <br></tt>  
  
  <tt> int MPI_Reduce_scatter_c(const void *sendbuf, void *recvbuf, const MPI_Count recvcounts[], MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) <br></tt>  
  <b> Fortran 2008 binding</b><br>  <tt> MPI_Reduce_scatter(sendbuf, recvbuf, recvcounts, datatype, op, comm, ierror) <br> TYPE(*), DIMENSION(..), INTENT(IN) :: <span style="white-space:nowrap">sendbuf</span><br>TYPE(*), DIMENSION(..) :: <span style="white-space:nowrap">recvbuf</span><br>INTEGER, INTENT(IN) :: <span style="white-space:nowrap">recvcounts(*)</span><br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">datatype</span><br>TYPE(MPI_Op), INTENT(IN) :: <span style="white-space:nowrap">op</span><br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span><br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <tt> MPI_Reduce_scatter(sendbuf, recvbuf, recvcounts, datatype, op, comm, ierror) !(_c) <br> TYPE(*), DIMENSION(..), INTENT(IN) :: <span style="white-space:nowrap">sendbuf</span><br>TYPE(*), DIMENSION(..) :: <span style="white-space:nowrap">recvbuf</span><br>INTEGER(KIND=MPI_COUNT_KIND), INTENT(IN) :: <span style="white-space:nowrap">recvcounts(*)</span><br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">datatype</span><br>TYPE(MPI_Op), INTENT(IN) :: <span style="white-space:nowrap">op</span><br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span><br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <b> Fortran binding</b><br>  <tt> MPI_REDUCE_SCATTER(SENDBUF, RECVBUF, RECVCOUNTS, DATATYPE, OP, COMM, IERROR) <br> &lt;type&gt; <span style="white-space:nowrap">SENDBUF(*)</span>, <span style="white-space:nowrap">RECVBUF(*)</span><br>INTEGER <span style="white-space:nowrap">RECVCOUNTS(*)</span>, <span style="white-space:nowrap">DATATYPE</span>, <span style="white-space:nowrap">OP</span>, <span style="white-space:nowrap">COMM</span>, <span style="white-space:nowrap">IERROR</span> <br></tt>  
<P> 
If <font face="sans-serif"> comm</font> is an intra-communicator,  
<font face="sans-serif"> MPI_REDUCE_SCATTER</font> first  
performs a global, element-wise reduction on vectors of  
<i><span style="font-family:sans-serif;"> count</span> = &sum;<SUB>i=0</SUB><SUP>n-1</SUP> <span style="font-family:sans-serif;"> recvcounts[i]</span></i> elements in the send  
buffers defined by <font face="sans-serif"> sendbuf</font>, <font face="sans-serif"> count</font> and  
<font face="sans-serif"> datatype</font>, using the operation <font face="sans-serif"> op</font>, where <font face="sans-serif"> n</font> is the number of  
<font face="sans-serif"> MPI</font> processes in the group of <font face="sans-serif"> comm</font>. The routine is called by all group  
members using the same arguments for <font face="sans-serif"> recvcounts</font>,  
<font face="sans-serif"> datatype</font>, <font face="sans-serif"> op</font> and <font face="sans-serif"> comm</font>.  
The resulting vector is treated as <font face="sans-serif"> n</font> consecutive blocks where the number  
of elements of the <font face="sans-serif"> i</font>-th block is <font face="sans-serif"> recvcounts[i]</font>. The blocks are scattered  
to the <font face="sans-serif"> MPI</font> processes of the group. The <font face="sans-serif"> i</font>-th block  
is sent to <font face="sans-serif"> MPI</font> process <font face="sans-serif"> i</font> and stored in the  
receive buffer defined by <font face="sans-serif"> recvbuf, recvcounts[i]</font> and  
<font face="sans-serif"> datatype</font>.  
<P> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
The <font face="sans-serif"> MPI_REDUCE_SCATTER</font>  
routine is functionally equivalent to  
an  
<font face="sans-serif"> MPI_REDUCE</font>  
collective  
operation  
with <font face="sans-serif"> count</font> equal to  
the sum of <font face="sans-serif"> recvcounts[i]</font> followed by  
<font face="sans-serif"> MPI_SCATTERV</font> with <font face="sans-serif"> sendcounts</font> equal to <font face="sans-serif"> recvcounts</font>.  
However, a direct implementation may run faster.  
 (<em> End of advice to implementors.</em>) <br> 
The ``in place'' option  for intra-communicators is specified by passing  
<font face="sans-serif"> MPI_IN_PLACE</font> in  
the <font face="sans-serif"> sendbuf</font> argument.  
In this case, the input data is taken from the receive  
buffer. It is not required to specify the ``in  
place'' option on all <font face="sans-serif"> MPI</font> processes, since the <font face="sans-serif"> MPI</font> processes for which  
<font face="sans-serif"> recvcounts[i]</font><font face="sans-serif">  =0</font> may not have allocated a receive buffer.  
<P> 
If <font face="sans-serif"> comm</font> is an inter-communicator, then the result of the reduction  
of the data provided by <font face="sans-serif"> MPI</font> processes in one  
group (group A) is scattered among <font face="sans-serif"> MPI</font> processes in  
the other group (group B), and vice  
versa.  Within each group, all <font face="sans-serif"> MPI</font> processes provide the same  
<font face="sans-serif"> recvcounts</font> argument, and provide input vectors of <i><span style="font-family:sans-serif;"> count</span> = &sum;<SUB>i=0</SUB><SUP>n-1</SUP> <span style="font-family:sans-serif;"> recvcounts[i]</span></i>  
elements stored in the send buffers, where <font face="sans-serif"> n</font> is the size of the group.  
The resulting vector from the other group is scattered in blocks of  
<font face="sans-serif"> recvcounts[i]</font> elements among the <font face="sans-serif"> MPI</font> processes in the group. The number of  
elements <font face="sans-serif"> count</font> must be the same for the two groups.  
<P> 
 
<br> 
<em> Rationale.</em>  
<P> 
The last restriction is needed so that the length of the send  
buffer can be determined by the sum of the local <font face="sans-serif"> recvcounts</font> entries.  
Otherwise, communication is needed to figure out how many elements  
are reduced.  
 (<em> End of rationale.</em>) <br> 

<P>
<hr>
<a href="node141.htm#Node141"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node140.htm#Node140"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node143.htm#Node143"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node140.htm#Node140"> Reduce-Scatter</a>
<b>Next: </b><a href="node143.htm#Node143"> Scan</a>
<b>Previous: </b><a href="node141.htm#Node141"> <font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font></a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
