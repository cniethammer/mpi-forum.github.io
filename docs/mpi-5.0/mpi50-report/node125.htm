<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-coll/coll-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Examples using <font face="sans-serif"> MPI_GATHER</font> and <font face="sans-serif"> MPI_GATHERV</font></title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node125">7.5.1. Examples using  MPI_GATHER and  MPI_GATHERV</span></h2>
<a href="node124.htm#Node124"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node124.htm#Node124"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node126.htm#Node126"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node124.htm#Node124"> Gather</a>
<b>Next: </b><a href="node126.htm#Node126"> Scatter</a>
<b>Previous: </b><a href="node124.htm#Node124"> Gather</a>
<p>
The examples in this section use intra-communicators.  
<P> 
<br><b> Example</b>  
  
Gather 100 <tt>int</tt>s from every <font face="sans-serif"> MPI</font> process in group to the root. See  
Figure <a href="node125.htm#Figure7">7</a>.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize,sendarray[100]; 
<b>int</b> root, *rbuf; 
... 
<b>MPI_Comm_size</b>(comm, &amp;gsize); 
rbuf = (<b>int</b> *)malloc(gsize*100*<b>sizeof</b>(<b>int</b>)); 
<b>MPI_Gather</b>(sendarray, 100, <b>MPI_INT</b>, rbuf, 100, <b>MPI_INT</b>, root, comm); 
</tt></pre> 
  
  
<P> 
<br><b> Example</b>  
  
Previous example modified---only the root allocates memory for the  
receive buffer. The argument <font face="sans-serif"> rbuf</font> still must be initialized on all processes.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize,sendarray[100]; 
<b>int</b> root, myrank, *rbuf = NULL; 
... 
<b>MPI_Comm_rank</b>(comm, &amp;myrank); 
<b>if</b> (myrank == root) { 
   <b>MPI_Comm_size</b>(comm, &amp;gsize); 
   rbuf = (<b>int</b> *)malloc(gsize*100*<b>sizeof</b>(<b>int</b>)); 
} 
<b>MPI_Gather</b>(sendarray, 100, <b>MPI_INT</b>, rbuf, 100, <b>MPI_INT</b>, root, comm); 
</tt></pre> 
  
  
<P> 
  <div style="text-align:center"><P><img width=983 height=410 src="mycoll-fig2.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 7: </b><span id="Figure7">The root gathers 100 <tt>int</tt>s from 
    each <font face="sans-serif"> MPI</font> process in the group</span><P> 
  
    
<br><b> Example</b>  
  
Do the same as the previous example, but use a derived datatype.  Note that  
the type cannot be the entire set of <tt>gsize*100 int</tt>s since type matching  
is defined pairwise between the root and each <font face="sans-serif"> MPI</font> process in the gather.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize,sendarray[100]; 
<b>int</b> root, *rbuf; 
<b>MPI_Datatype</b> rtype; 
... 
<b>MPI_Comm_size</b>(comm, &amp;gsize); 
<b>MPI_Type_contiguous</b>(100, <b>MPI_INT</b>, &amp;rtype); 
<b>MPI_Type_commit</b>(&amp;rtype); 
rbuf = (<b>int</b> *)malloc(gsize*100*<b>sizeof</b>(<b>int</b>)); 
<b>MPI_Gather</b>(sendarray, 100, <b>MPI_INT</b>, rbuf, 1, rtype, root, comm); 
</tt></pre> 
  
  
<P> 
<br><b> Example</b>  
  
Now have each <font face="sans-serif"> MPI</font> process send 100 <tt>int</tt>s to the root, but place each set (of 100)  
<tt>stride int</tt>s apart at the receiving end. Use <font face="sans-serif"> MPI_GATHERV</font>  
and the <font face="sans-serif"> displs</font>  
argument to achieve this effect. Assume <tt>stride</tt> <i>&ge; 100</i>.  
See Figure <a href="node125.htm#Figure8">8</a>.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize,sendarray[100]; 
<b>int</b> root, *rbuf, stride; 
<b>int</b> *displs,i,*rcounts; 
 
... 
 
<b>MPI_Comm_size</b>(comm, &amp;gsize); 
rbuf = (<b>int</b> *)malloc(gsize*stride*<b>sizeof</b>(<b>int</b>)); 
displs = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
rcounts = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
<b>for</b> (i=0; i&lt;gsize; ++i) { 
    displs[i] = i*stride; 
    rcounts[i] = 100; 
} 
<b>MPI_Gatherv</b>(sendarray, 100, <b>MPI_INT</b>, rbuf, rcounts, displs, <b>MPI_INT</b>, 
            root, comm); 
</tt></pre> 
  
<P> 
Note that the program is erroneous if <tt>stride</tt> <i>&lt; 100</i>.  
  
<P> 
  <div style="text-align:center"><P><img width=983 height=410 src="mycoll-fig3.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 8: </b><span id="Figure8">The root gathers 100 
    <tt>int</tt>s from each <font face="sans-serif"> MPI</font> process 
  in the group, each set is placed <tt>stride int</tt>s apart</span><P> 
  
    
<br><b> Example</b>  
  
Same as Example <a href="node125.htm#Node125">Examples using <font face="sans-serif"> MPI_GATHER</font> and <font face="sans-serif"> MPI_GATHERV</font></a> on the receiving side, but send the  
100 <tt>int</tt>s from the 0th column of a  
100<i>&times;</i>150 <tt>int</tt> array, in C.  See Figure <a href="node125.htm#Figure9">9</a>.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize,sendarray[100][150]; 
<b>int</b> root, *rbuf, stride; 
<b>MPI_Datatype</b> stype; 
<b>int</b> *displs,i,*rcounts; 
 
... 
 
<b>MPI_Comm_size</b>(comm, &amp;gsize); 
rbuf = (<b>int</b> *)malloc(gsize*stride*<b>sizeof</b>(<b>int</b>)); 
displs = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
rcounts = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
<b>for</b> (i=0; i&lt;gsize; ++i) { 
    displs[i] = i*stride; 
    rcounts[i] = 100; 
} 
/* Create datatype <b>for</b> 1 column of array 
 */ 
<b>MPI_Type_vector</b>(100, 1, 150, <b>MPI_INT</b>, &amp;stype); 
<b>MPI_Type_commit</b>(&amp;stype); 
<b>MPI_Gatherv</b>(sendarray, 1, stype, rbuf, rcounts, displs, <b>MPI_INT</b>, 
            root, comm); 
</tt></pre> 
  
  
<P> 
  <div style="text-align:center"><P><img width=1158 height=523 src="mycoll-fig4.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 9: </b><span id="Figure9">The root gathers column 
    <tt>0</tt> of a 100$ x $150 
  C array, and each set is placed <tt>stride int</tt>s apart</span><P> 
  
    
<br><b> Example</b>  
  
<font face="sans-serif"> MPI</font> process <tt>i</tt> sends <tt>(100-i) int</tt>s from the <tt>i</tt>-th column of a  
100 <i>&times;</i> 150 <tt>int</tt> array, in C.  It is received into a buffer with stride,  
as in the previous two examples. See Figure <a href="node125.htm#Figure10">10</a>.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize,sendarray[100][150],*sptr; 
<b>int</b> root, *rbuf, stride, myrank; 
<b>MPI_Datatype</b> stype; 
<b>int</b> *displs,i,*rcounts; 
 
... 
 
<b>MPI_Comm_size</b>(comm, &amp;gsize); 
<b>MPI_Comm_rank</b>(comm, &amp;myrank); 
rbuf = (<b>int</b> *)malloc(gsize*stride*<b>sizeof</b>(<b>int</b>)); 
displs = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
rcounts = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
<b>for</b> (i=0; i&lt;gsize; ++i) { 
    displs[i] = i*stride; 
    rcounts[i] = 100-i;     /* note change from previous example */ 
} 
/* Create datatype <b>for</b> the column we are sending 
 */ 
<b>MPI_Type_vector</b>(100-myrank, 1, 150, <b>MPI_INT</b>, &amp;stype); 
<b>MPI_Type_commit</b>(&amp;stype); 
/* sptr is the address of start of "myrank" column 
 */ 
sptr = &amp;sendarray[0][myrank]; 
<b>MPI_Gatherv</b>(sptr, 1, stype, rbuf, rcounts, displs, <b>MPI_INT</b>, 
            root, comm); 
</tt></pre> 
  
<P> 
Note that a different amount of data is received from each <font face="sans-serif"> MPI</font> process.  
  
<P> 
  <div style="text-align:center"><P><img width=1158 height=523 src="mycoll-fig5.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 10: </b><span id="Figure10">The 
    root gathers <tt>100-i int</tt>s from 
  column <tt>i</tt> of a 100$ x $150 
  C array, and each set is placed <tt>stride int</tt>s apart</span><P> 
  
    
<br><b> Example</b>  
  
Same as Example <a href="node125.htm#Node125">Examples using <font face="sans-serif"> MPI_GATHER</font> and <font face="sans-serif"> MPI_GATHERV</font></a>, but done in a different way at the sending end.  
We create a datatype that causes the correct striding at the  
sending end so  
that  
we read a column of a C array.  
A similar thing was done in Example <a href="node112.htm#Node112">Examples</a>,  
Section <a href="node112.htm#Node112">Examples</a>.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize, sendarray[100][150], *sptr; 
<b>int</b> root, *rbuf, stride, myrank; 
<b>MPI_Datatype</b> stype; 
<b>int</b> *displs, i, *rcounts; 
 
... 
 
<b>MPI_Comm_size</b>(comm, &amp;gsize); 
<b>MPI_Comm_rank</b>(comm, &amp;myrank); 
rbuf = (<b>int</b> *)malloc(gsize*stride*<b>sizeof</b>(<b>int</b>)); 
displs = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
rcounts = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
<b>for</b> (i=0; i&lt;gsize; ++i) { 
    displs[i] = i*stride; 
    rcounts[i] = 100-i; 
} 
/* Create datatype <b>for</b> one <b>int</b>, with extent of entire row 
 */ 
<b>MPI_Type_create_resized</b>(<b>MPI_INT</b>, 0, 150*<b>sizeof</b>(<b>int</b>), &amp;stype); 
<b>MPI_Type_commit</b>(&amp;stype); 
sptr = &amp;sendarray[0][myrank]; 
<b>MPI_Gatherv</b>(sptr, 100-myrank, stype, rbuf, rcounts, displs, <b>MPI_INT</b>, 
            root, comm); 
</tt></pre> 
  
  
<P> 
<br><b> Example</b>  
  
Same as Example <a href="node125.htm#Node125">Examples using <font face="sans-serif"> MPI_GATHER</font> and <font face="sans-serif"> MPI_GATHERV</font></a> at sending side, but  
at receiving side we make the  
stride between received blocks vary from block to block.  
See Figure <a href="node125.htm#Figure11">11</a>.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize,sendarray[100][150],*sptr; 
<b>int</b> root, *rbuf, *stride, myrank, bufsize; 
<b>MPI_Datatype</b> stype; 
<b>int</b> *displs,i,*rcounts,offset; 
 
... 
 
<b>MPI_Comm_size</b>(comm, &amp;gsize); 
<b>MPI_Comm_rank</b>(comm, &amp;myrank); 
 
stride = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
... 
/* stride[i] <b>for</b> i = 0 to gsize-1 is set somehow 
 */ 
 
/* set up displs and rcounts vectors first 
 */ 
displs = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
rcounts = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
offset = 0; 
<b>for</b> (i=0; i&lt;gsize; ++i) { 
    displs[i] = offset; 
    offset += stride[i]; 
    rcounts[i] = 100-i; 
} 
/* the required buffer size <b>for</b> rbuf is now easily obtained 
 */ 
bufsize = displs[gsize-1]+rcounts[gsize-1]; 
rbuf = (<b>int</b> *)malloc(bufsize*<b>sizeof</b>(<b>int</b>)); 
/* Create datatype <b>for</b> the column we are sending 
 */ 
<b>MPI_Type_vector</b>(100-myrank, 1, 150, <b>MPI_INT</b>, &amp;stype); 
<b>MPI_Type_commit</b>(&amp;stype); 
sptr = &amp;sendarray[0][myrank]; 
<b>MPI_Gatherv</b>(sptr, 1, stype, rbuf, rcounts, displs, <b>MPI_INT</b>, 
            root, comm); 
</tt></pre> 
  
  
<P> 
  <div style="text-align:center"><P><img width=1158 height=523 src="mycoll-fig6.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 11: </b><span id="Figure11">The root gathers <tt>100-i int</tt>s from 
  column <tt>i</tt> of a 100$ x $150 
  C array, and each set is placed <tt>stride[i] int</tt>s apart (a varying 
  stride)</span><P> 
  
    
<br><b> Example</b>  
  
<font face="sans-serif"> MPI</font> process <tt>i</tt> sends <tt>num int</tt>s from the <tt>i</tt>-th column of a  
100 <i>&times;</i> 150 <tt>int</tt> array, in C.  The complicating factor is that  
the various values of <tt>num</tt> are not known to <tt>root</tt>, so a  
separate gather must first be run to find these out.  The data is  
placed contiguously at the receiving end.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Comm</b> comm; 
<b>int</b> gsize,sendarray[100][150],*sptr; 
<b>int</b> root, *rbuf, myrank; 
<b>MPI_Datatype</b> stype; 
<b>int</b> *displs,i,*rcounts,num; 
 
... 
 
<b>MPI_Comm_size</b>(comm, &amp;gsize); 
<b>MPI_Comm_rank</b>(comm, &amp;myrank); 
 
/* First, gather nums to root 
 */ 
rcounts = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
<b>MPI_Gather</b>(&amp;num, 1, <b>MPI_INT</b>, rcounts, 1, <b>MPI_INT</b>, root, comm); 
/* root now has correct rcounts, using these we set displs[] so that 
 * data is placed contiguously (or concatenated) at the receiving end 
 */ 
displs = (<b>int</b> *)malloc(gsize*<b>sizeof</b>(<b>int</b>)); 
displs[0] = 0; 
<b>for</b> (i=1; i&lt;gsize; ++i) { 
    displs[i] = displs[i-1]+rcounts[i-1]; 
} 
/* And, create receive buffer 
 */ 
rbuf = (<b>int</b> *)malloc(gsize*(displs[gsize-1]+rcounts[gsize-1]) 
                                                      *<b>sizeof</b>(<b>int</b>)); 
/* Create datatype <b>for</b> one <b>int</b>, with extent of entire row 
 */ 
<b>MPI_Type_create_resized</b>(<b>MPI_INT</b>, 0, 150*<b>sizeof</b>(<b>int</b>), &amp;stype); 
<b>MPI_Type_commit</b>(&amp;stype); 
sptr = &amp;sendarray[0][myrank]; 
<b>MPI_Gatherv</b>(sptr, num, stype, rbuf, rcounts, displs, <b>MPI_INT</b>, 
            root, comm); 
</tt></pre> 
  
  
<P> 

<P>
<hr>
<a href="node124.htm#Node124"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node124.htm#Node124"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node126.htm#Node126"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node124.htm#Node124"> Gather</a>
<b>Next: </b><a href="node126.htm#Node126"> Scatter</a>
<b>Previous: </b><a href="node124.htm#Node124"> Gather</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
