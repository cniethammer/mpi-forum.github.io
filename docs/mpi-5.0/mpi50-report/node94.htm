<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-part/part-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Partition Communication with Threads/Tasks Using OpenMP 4.0 or later</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node94">5.3.1. Partition Communication with Threads/Tasks Using OpenMP 4.0 or later</span></h2>
<a href="node93.htm#Node93"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node93.htm#Node93"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node95.htm#Node95"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node93.htm#Node93"> Partitioned Communication Examples</a>
<b>Next: </b><a href="node95.htm#Node95"> Send-only Partitioning Example with Tasks and OpenMP version 4.0 or later</a>
<b>Previous: </b><a href="node93.htm#Node93"> Partitioned Communication Examples</a>
<p>
  
In the following the equal partitioning on send-side and receive-side Example <a href="node89.htm#Node89">Semantics of Partitioned Point-to-Point Communication</a> is extended  
to utilize threads. In this case, the receive-side uses the same  
number of partitions as the send-side as in the previous example, but this  
example uses multiple threads on the send-side. Note that the  
<font face="sans-serif"> MPI_PSEND_INIT</font> and <font face="sans-serif"> MPI_PRECV_INIT</font> functions match  
each other like in the previous example.  
<P> 
<br><b> Example</b>  
Equal partitioning on send-side and receive-side using threads.  
<br> 
<pre style="background-color:#EFEFEF"><tt><b>#include</b> &lt;stdlib.h&gt; 
<b>#include</b> "mpi.h" 
<b>#define</b> NUM_THREADS 8 
<b>#define</b> PARTITIONS 8 
<b>#define</b> PARTLENGTH 16 
<b>int</b> main(<b>int</b> argc, <b>char</b> *argv[]) /* same send/recv partitioning */ 
{ 
  <b>double</b> message[PARTITIONS*PARTLENGTH]; 
  <b>int</b> partitions = PARTITIONS; 
  <b>int</b> partlength = PARTLENGTH; 
  <b>int</b> count = 1, source = 0, dest = 1, tag = 1, flag = 0; 
  <b>int</b> myrank; 
  <b>int</b> provided; 
  <b>MPI_Request</b> request; 
  <b>MPI_Info</b> info = <b>MPI_INFO_NULL</b>; 
  <b>MPI_Datatype</b> xfer_type; 
  <b>MPI_Init_thread</b>(&amp;argc, &amp;argv, <b>MPI_THREAD_MULTIPLE</b>, &amp;provided); 
  <b>if</b> (provided &lt; <b>MPI_THREAD_MULTIPLE</b>) 
     <b>MPI_Abort</b>(<b>MPI_COMM_WORLD</b>, EXIT_FAILURE); 
  <b>MPI_Comm_rank</b>(<b>MPI_COMM_WORLD</b>, &amp;myrank); 
  <b>MPI_Type_contiguous</b>(partlength, <b>MPI_DOUBLE</b>, &amp;xfer_type); 
  <b>MPI_Type_commit</b>(&amp;xfer_type); 
  <b>if</b> (myrank == 0) 
  { 
     <b>MPI_Psend_init</b>(message, partitions, count, xfer_type, dest, tag, 
                    <b>MPI_COMM_WORLD</b>, info, &amp;request); 
     <b>MPI_Start</b>(&amp;request); 
 
     <b>#pragma</b> omp parallel <b>for</b> shared(request) num_threads(NUM_THREADS) 
     <b>for</b> (<b>int</b> i=0; i&lt;partitions; i++) 
     { 
        /* compute and fill partition #i, then mark ready: */ 
        <b>MPI_Pready</b>(i, request); 
     } 
     <b>while</b>(!flag) 
     { 
        /* Do useful work */ 
        <b>MPI_Test</b>(&amp;request, &amp;flag, <b>MPI_STATUS_IGNORE</b>); 
        /* Do useful work */ 
     } 
     <b>MPI_Request_free</b>(&amp;request); 
  } 
  <b>else</b> <b>if</b> (myrank == 1) 
  { 
     <b>MPI_Precv_init</b>(message, partitions, count, xfer_type, source, tag, 
                    <b>MPI_COMM_WORLD</b>, info, &amp;request); 
     <b>MPI_Start</b>(&amp;request); 
     <b>while</b>(!flag) 
     { 
        /* Do useful work */ 
        <b>MPI_Test</b>(&amp;request, &amp;flag, <b>MPI_STATUS_IGNORE</b>); 
        /* Do useful work */ 
     } 
     <b>MPI_Request_free</b>(&amp;request); 
  } 
  <b>MPI_Finalize</b>(); 
  <b>return</b> 0; 
} 
</tt></pre> 
  
  
<P> 

<P>
<hr>
<a href="node93.htm#Node93"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node93.htm#Node93"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node95.htm#Node95"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node93.htm#Node93"> Partitioned Communication Examples</a>
<b>Next: </b><a href="node95.htm#Node95"> Send-only Partitioning Example with Tasks and OpenMP version 4.0 or later</a>
<b>Previous: </b><a href="node93.htm#Node93"> Partitioned Communication Examples</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
