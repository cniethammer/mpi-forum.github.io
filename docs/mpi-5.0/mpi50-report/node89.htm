<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-part/part-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Semantics of Partitioned Point-to-Point Communication</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h1><span id="Node89">5.2. Semantics of Partitioned Point-to-Point Communication</span></h1>
<a href="node88.htm#Node88"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node87.htm#Node87"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node90.htm#Node90"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node87.htm#Node87"> Partitioned Point-to-Point Communication</a>
<b>Next: </b><a href="node90.htm#Node90"> Communication Initialization and Starting with Partitioning</a>
<b>Previous: </b><a href="node88.htm#Node88"> Introduction</a>
<p>
  
   
<font face="sans-serif"> MPI</font> guarantees certain general properties of  
partitioned point-to-point communication progress,  
which are described in this section.  
<P> 
Persistent communications use opaque <font face="sans-serif"> MPI_REQUEST</font> objects  
as described in Section <a href="node55.htm#Node55">Point-to-Point Communication</a>. Partitioned communication  
uses these same semantics for <font face="sans-serif"> MPI_REQUEST</font> objects.  
<P> 
Partitioned communication provides fine-grained transfers on either or  
both sides of a send-receive operation described by requests.  
Persistent communication semantics are ideal for partitioned  
communication: they provide <font face="sans-serif"> MPI_PSEND_INIT</font> and  
<font face="sans-serif"> MPI_PRECV_INIT</font> functions that allow partitioned  
communication setup to occur prior to message transfers. Partitioned  
communication initialization functions are local.  The partitioned  
communication initialization includes inputs on the number of  
user-visible partitions on the send-side and receive-side, which may  
differ.  Valid partitioned communication operations must have  
one or more partitions specified.  
<P> 
Once an <font face="sans-serif"> MPI_PSEND_INIT</font> call has been made, the user may start the  
operation with a call to a starting procedure and complete the  
operation with one <font face="sans-serif"> MPI_PREADY</font> call for every  
send partition followed by a call to a completing  
procedure. A call to <font face="sans-serif"> MPI_PREADY</font> notifies the  
<font face="sans-serif"> MPI</font> library that a specified portion of the data buffer (a specific partition) is ready to be sent.  
Notification of partial completion can be done  
via fine-grained <font face="sans-serif"> MPI_PARRIVED</font> calls at the receiver before a final  
<font face="sans-serif"> MPI_TEST</font>/<font face="sans-serif"> MPI_WAIT</font> on the request itself; the latter  
represents overall operation completion upon success.    
A full set of methods  
for starting and completing partitioned communication is given in the  
following sections.  
<P> 
 
<br> 
<em> Advice to users.</em>  
<P> 
Having a large number of receiver-side partitions can increase overheads   
as the completion mechanism may need to work with finer-grained notifications. Using a small  
number of receiver-side partitions <em> may</em> provide higher performance.  
<P> 
A large number of sender-side partitions may be aggregated by an <font face="sans-serif"> MPI</font> implementation,  
making performance concerns of a large number of sender-side partitions potentially less impactful  
than receiver-side granularity.   
 (<em> End of advice to users.</em>) <br> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
It is expected that an <font face="sans-serif"> MPI</font> implementation will attempt to balance  
latency and aggregation for  
data transfers for the requested partition counts on the sender-side and receiver-side  
to allow optimization for different hardware. A high quality implementation may   
perform significant optimizations to enhance performance in this way; they may, for example,  
resize the data transfers of the partitions to combine partitions in fractional  
partition sizes (e.g., 2.5 partitions in a single data transfer).   
 (<em> End of advice to implementors.</em>) <br> 
Example <a href="node89.htm#Node89">Semantics of Partitioned Point-to-Point Communication</a> shows a simple partitioned transfer  
in which the sender-side and receiver-side partitioning is identical in partition count.  
<P> 
<br><b> Example</b>  
Simple partitioned communication example.  
<br> 
<pre style="background-color:#EFEFEF"><tt><b>#include</b> &lt;stdlib.h&gt; 
<b>#include</b> "mpi.h" 
<b>#define</b> PARTITIONS 8 
<b>#define</b> COUNT 5 
<b>int</b> main(<b>int</b> argc, <b>char</b> *argv[]) 
{ 
  <b>double</b> message[PARTITIONS*COUNT]; 
  <b>int</b> partitions = PARTITIONS; 
  <b>int</b> source = 0, dest = 1, tag = 1, flag = 0; 
  <b>int</b> myrank, i; 
  <b>MPI_Request</b> request; 
  <b>MPI_Init</b>(&amp;argc, &amp;argv); 
  <b>MPI_Comm_rank</b>(<b>MPI_COMM_WORLD</b>, &amp;myrank); 
  <b>if</b> (myrank == 0) 
  { 
     <b>MPI_Psend_init</b>(message, partitions, COUNT, <b>MPI_DOUBLE</b>, dest, tag, 
                    <b>MPI_COMM_WORLD</b>, <b>MPI_INFO_NULL</b>, &amp;request); 
     <b>MPI_Start</b>(&amp;request); 
     <b>for</b>(i = 0; i &lt; partitions; ++i) 
     { 
        /* compute and fill partition #i, then mark ready: */ 
        <b>MPI_Pready</b>(i, request); 
     } 
     <b>while</b>(!flag) 
     { 
        /* <b>do</b> useful work #1 */ 
        <b>MPI_Test</b>(&amp;request, &amp;flag, <b>MPI_STATUS_IGNORE</b>); 
        /* <b>do</b> useful work #2 */ 
     } 
     <b>MPI_Request_free</b>(&amp;request); 
  } 
  <b>else</b> <b>if</b> (myrank == 1) 
  { 
     <b>MPI_Precv_init</b>(message, partitions, COUNT, <b>MPI_DOUBLE</b>, source, tag, 
                    <b>MPI_COMM_WORLD</b>, <b>MPI_INFO_NULL</b>, &amp;request); 
     <b>MPI_Start</b>(&amp;request); 
     <b>while</b>(!flag) 
     { 
        /* <b>do</b> useful work #1 */ 
        <b>MPI_Test</b>(&amp;request, &amp;flag, <b>MPI_STATUS_IGNORE</b>); 
        /* <b>do</b> useful work #2 */ 
     } 
     <b>MPI_Request_free</b>(&amp;request); 
  } 
  <b>MPI_Finalize</b>(); 
  <b>return</b> 0; 
} 
</tt></pre> 
  
  
<P> 
 
<br> 
<em> Rationale.</em>  
<P> 
Partitioned communication is designed to provide opportunities for <font face="sans-serif"> MPI</font> implementations to  
optimize data transfers. <font face="sans-serif"> MPI</font> is free to choose how many transfers to do within a partitioned communication send  
independent of how many partitions are reported as ready to <font face="sans-serif"> MPI</font> through  
<font face="sans-serif"> MPI_PREADY</font> calls.  
Aggregation of partitions is permitted but not required.  Ordering of partitions  is permitted but not required.  
A naive implementation can simply  
wait for the entire message buffer to be marked ready before any transfer(s) occur and  
could wait until the completion function is called on a request before transferring data.  
However, this modality of communication gives <font face="sans-serif"> MPI</font> implementations far more flexibility in data  
movement than  
nonpartitioned communications.  
 (<em> End of rationale.</em>) <br> 
<ul> 
</ul> 

<P>
<hr>
<a href="node88.htm#Node88"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node87.htm#Node87"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node90.htm#Node90"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node87.htm#Node87"> Partitioned Point-to-Point Communication</a>
<b>Next: </b><a href="node90.htm#Node90"> Communication Initialization and Starting with Partitioning</a>
<b>Previous: </b><a href="node88.htm#Node88"> Introduction</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
