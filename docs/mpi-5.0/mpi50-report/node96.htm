<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-part/part-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Send and Receive Partitioning Example with OpenMP version 4.0 or later</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node96">5.3.3. Send and Receive Partitioning Example with OpenMP version 4.0 or later</span></h2>
<a href="node95.htm#Node95"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node93.htm#Node93"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node97.htm#Node97"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node93.htm#Node93"> Partitioned Communication Examples</a>
<b>Next: </b><a href="node97.htm#Node97"> Datatypes</a>
<b>Previous: </b><a href="node95.htm#Node95"> Send-only Partitioning Example with Tasks and OpenMP version 4.0 or later</a>
<p>
This example demonstrates receive-side partial completion notification  
using more than one partition per receive-side thread. It uses a naive flag   
based method to test for multiple completed partitions per thread.  
Note that this means that some threads may be busy polling for completion of  
assigned partitions when partitions are available to work on that were not   
assigned to the polling threads in this example. More advanced work stealing methods  
could be employed for greater efficiency.  
Like previous examples, it also demonstrates send-side production of  
input to part of an overall buffer. This example also uses different send-side and  
receive-side partitioning.   
<P> 
<br><b> Example</b>  
  
Partitioned communication receive-side partial completion.<br>  
<br> 
<pre style="background-color:#EFEFEF"><tt><b>#include</b> &lt;stdlib.h&gt; 
<b>#include</b> "mpi.h" 
<b>#define</b> NUM_THREADS 64 
<b>#define</b> PARTITIONS NUM_THREADS 
<b>#define</b> PARTLENGTH 16 
<b>#define</b> MESSAGE_LENGTH PARTITIONS*PARTLENGTH 
<b>int</b> main(<b>int</b> argc, <b>char</b> *argv[]) /* send-side partitioning */ 
{ 
  <b>double</b> message[MESSAGE_LENGTH]; 
  <b>int</b> send_partitions = PARTITIONS, 
      send_partlength = PARTLENGTH, 
      recv_partitions = PARTITIONS*2, 
      recv_partlength = PARTLENGTH/2; 
  <b>int</b> source = 0, dest = 1, tag = 1, flag = 0; 
  <b>int</b> myrank; 
  <b>int</b> provided; 
  <b>MPI_Request</b> request; 
  <b>MPI_Info</b> info = <b>MPI_INFO_NULL</b>; 
  <b>MPI_Datatype</b> send_type; 
  <b>MPI_Init_thread</b>(&amp;argc, &amp;argv, <b>MPI_THREAD_MULTIPLE</b>, &amp;provided); 
  <b>if</b> (provided &lt; <b>MPI_THREAD_MULTIPLE</b>) 
     <b>MPI_Abort</b>(<b>MPI_COMM_WORLD</b>, EXIT_FAILURE); 
  <b>MPI_Comm_rank</b>(<b>MPI_COMM_WORLD</b>, &amp;myrank); 
  <b>MPI_Type_contiguous</b>(send_partlength, <b>MPI_DOUBLE</b>, &amp;send_type); 
  <b>MPI_Type_commit</b>(&amp;send_type); 
 
  <b>if</b> (myrank == 0) 
  { 
     <b>MPI_Psend_init</b>(message, send_partitions, 1, send_type, dest, tag, 
                    <b>MPI_COMM_WORLD</b>, info, &amp;request); 
     <b>MPI_Start</b>(&amp;request); 
     <b>#pragma</b> omp parallel <b>for</b> shared(request) \ 
                              firstprivate(send_partitions) \ 
                              num_threads(NUM_THREADS) 
     <b>for</b> (<b>int</b> i=0; i&lt;send_partitions; i++) 
     { 
        /* compute and fill partition #i, then mark ready: */ 
        <b>MPI_Pready</b>(i, request); 
     } 
     <b>while</b>(!flag) 
     { 
        /* Do useful work */ 
        <b>MPI_Test</b>(&amp;request, &amp;flag, <b>MPI_STATUS_IGNORE</b>); 
        /* Do useful work */ 
     } 
     <b>MPI_Request_free</b>(&amp;request); 
  } 
  <b>else</b> <b>if</b> (myrank == 1) 
  { 
     <b>MPI_Precv_init</b>(message, recv_partitions, recv_partlength, 
                    <b>MPI_DOUBLE</b>, source, tag, <b>MPI_COMM_WORLD</b>, info, 
                    &amp;request); 
     <b>MPI_Start</b>(&amp;request); 
     <b>#pragma</b> omp parallel <b>for</b> shared(request) \ 
                              firstprivate(recv_partitions) \ 
                              num_threads(NUM_THREADS) 
     <b>for</b> (<b>int</b> j=0; j&lt;recv_partitions; j+=2) 
     { 
        <b>int</b> part_flag = 0; 
        <b>int</b> part1_complete = 0; 
        <b>int</b> part2_complete = 0; 
        <b>while</b>(part1_complete == 0 || part2_complete == 0) 
        { 
           /* test partition #j and #j+1 */ 
           <b>MPI_Parrived</b>(request, j, &amp;part_flag); 
           <b>if</b>(part_flag &amp;&amp; part1_complete == 0) 
           { 
              part1_complete++; 
              /* Do work using partition j data */ 
           } 
           <b>MPI_Parrived</b>(request, j+1, &amp;part_flag); 
           <b>if</b>(part_flag &amp;&amp; part2_complete == 0) 
           { 
              part2_complete++; 
              /* Do work using partition j+1 */ 
           } 
        } 
     } 
     <b>while</b>(!flag) 
     { 
        /* Do useful work */ 
        <b>MPI_Test</b>(&amp;request, &amp;flag, <b>MPI_STATUS_IGNORE</b>); 
        /* Do useful work */ 
     } 
     <b>MPI_Request_free</b>(&amp;request); 
  } 
  <b>MPI_Finalize</b>(); 
  <b>return</b> 0; 
} 
</tt></pre> 
  
  
<P> 
<P> 
  

<P>
<hr>
<a href="node95.htm#Node95"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node93.htm#Node93"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node97.htm#Node97"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node93.htm#Node93"> Partitioned Communication Examples</a>
<b>Next: </b><a href="node97.htm#Node97"> Datatypes</a>
<b>Previous: </b><a href="node95.htm#Node95"> Send-only Partitioning Example with Tasks and OpenMP version 4.0 or later</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
