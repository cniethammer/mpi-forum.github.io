<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-part/part-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Send-only Partitioning Example with Tasks and OpenMP version 4.0 or later</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node95">5.3.2. Send-only Partitioning Example with Tasks and OpenMP version 4.0 or later</span></h2>
<a href="node94.htm#Node94"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node93.htm#Node93"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node96.htm#Node96"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node93.htm#Node93"> Partitioned Communication Examples</a>
<b>Next: </b><a href="node96.htm#Node96"> Send and Receive Partitioning Example with OpenMP version 4.0 or later</a>
<b>Previous: </b><a href="node94.htm#Node94"> Partition Communication with Threads/Tasks Using OpenMP 4.0 or later</a>
<p>
The previous example is tailored specifically for send-side partitioning using threads.  
This is an example where parallel task producers produce  
input to part of an overall buffer; they complete in any order and contribute  
to the overall buffer.   
<br><b> Example</b>  
Parallel task producers for partitioned communication using threads.  
<br> 
<pre style="background-color:#EFEFEF"><tt><b>#include</b> &lt;stdlib.h&gt; 
<b>#include</b> "mpi.h" 
<b>#define</b> NUM_THREADS 8 
<b>#define</b> NUM_TASKS 64 
<b>#define</b> PARTITIONS NUM_TASKS 
<b>#define</b> PARTLENGTH 16 
<b>#define</b> MESSAGE_LENGTH PARTITIONS*PARTLENGTH 
<b>int</b> main(<b>int</b> argc, <b>char</b> *argv[]) /* send-side partitioning */ 
{ 
  <b>double</b> message[MESSAGE_LENGTH]; 
  <b>int</b> send_partitions = PARTITIONS, 
      send_partlength = PARTLENGTH, 
      recv_partitions = 1, 
      recv_partlength = PARTITIONS*PARTLENGTH; 
  <b>int</b> count = 1, source = 0, dest = 1, tag = 1, flag = 0; 
  <b>int</b> myrank; 
  <b>int</b> provided; 
  <b>MPI_Request</b> request; 
  <b>MPI_Info</b> info = <b>MPI_INFO_NULL</b>; 
  <b>MPI_Datatype</b> send_type; 
  <b>MPI_Init_thread</b>(&amp;argc, &amp;argv, <b>MPI_THREAD_MULTIPLE</b>, &amp;provided); 
  <b>if</b> (provided &lt; <b>MPI_THREAD_MULTIPLE</b>) 
     <b>MPI_Abort</b>(<b>MPI_COMM_WORLD</b>, EXIT_FAILURE); 
  <b>MPI_Comm_rank</b>(<b>MPI_COMM_WORLD</b>, &amp;myrank); 
  <b>MPI_Type_contiguous</b>(send_partlength, <b>MPI_DOUBLE</b>, &amp;send_type); 
  <b>MPI_Type_commit</b>(&amp;send_type); 
 
  <b>if</b> (myrank == 0) 
  { 
     <b>MPI_Psend_init</b>(message, send_partitions, count, send_type, dest, tag, 
                    <b>MPI_COMM_WORLD</b>, info, &amp;request); 
     <b>MPI_Start</b>(&amp;request); 
 
     <b>#pragma</b> omp parallel shared(request) num_threads(NUM_THREADS) 
     { 
        <b>#pragma</b> omp single 
        { 
           /* single thread creates 64 tasks to be executed by 8 threads */ 
           <b>for</b> (<b>int</b> partition_num=0;partition_num&lt;NUM_TASKS;partition_num++) 
           { 
              <b>#pragma</b> omp task firstprivate(partition_num) 
              { 
                 /* compute and fill partition #partition_num, then mark 
                 ready: */ 
                 /* buffer is filled in arbitrary order from each task */ 
                 <b>MPI_Pready</b>(partition_num, request); 
              } /*end task*/ 
           } /* end <b>for</b> */ 
        } /* end single */ 
     } /* end parallel */ 
     <b>while</b>(!flag) 
     { 
        /* Do useful work */ 
        <b>MPI_Test</b>(&amp;request, &amp;flag, <b>MPI_STATUS_IGNORE</b>); 
        /* Do useful work */ 
     } 
     <b>MPI_Request_free</b>(&amp;request); 
  } 
  <b>else</b> <b>if</b> (myrank == 1) 
  { 
     <b>MPI_Precv_init</b>(message, recv_partitions, recv_partlength, <b>MPI_DOUBLE</b>, 
                    source, tag, <b>MPI_COMM_WORLD</b>, info, &amp;request); 
 
     <b>MPI_Start</b>(&amp;request); 
     <b>while</b>(!flag) 
     { 
        /* Do useful work */ 
        <b>MPI_Test</b>(&amp;request, &amp;flag, <b>MPI_STATUS_IGNORE</b>); 
        /* Do useful work */ 
     } 
     <b>MPI_Request_free</b>(&amp;request); 
  } 
  <b>MPI_Finalize</b>(); 
  <b>return</b> 0; 
} 
</tt></pre> 
  
  
<P> 

<P>
<hr>
<a href="node94.htm#Node94"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node93.htm#Node93"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node96.htm#Node96"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node93.htm#Node93"> Partitioned Communication Examples</a>
<b>Next: </b><a href="node96.htm#Node96"> Send and Receive Partitioning Example with OpenMP version 4.0 or later</a>
<b>Previous: </b><a href="node94.htm#Node94"> Partition Communication with Threads/Tasks Using OpenMP 4.0 or later</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
