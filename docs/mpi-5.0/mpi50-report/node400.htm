<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-io/io-2-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Double Buffering with Split Collective I/O</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node400">15.9.1. Double Buffering with Split Collective I/O</span></h2>
<a href="node399.htm#Node399"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node399.htm#Node399"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node401.htm#Node401"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node399.htm#Node399"> Examples</a>
<b>Next: </b><a href="node401.htm#Node401"> Subarray Filetype Constructor</a>
<b>Previous: </b><a href="node399.htm#Node399"> Examples</a>
<p>
  
<br><b> Example</b>  
  
  
This example shows how to overlap computation and output.  
The computation is performed by the function <tt>compute_buffer()</tt>.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt>/*========================================================================= 
 * 
 * Function:            double_buffer 
 * 
 * Synopsis: 
 *      <b>void</b> double_buffer( 
 *              <b>MPI_File</b> fh,                            ** IN 
 *              <b>MPI_Datatype</b> buftype,                   ** IN 
 *              <b>int</b> bufcount                            ** IN 
 *      ) 
 * 
 * Description: 
 *      Performs the steps to overlap computation with a collective write 
 *      by using a <b>double</b>-buffering technique. 
 * 
 * Parameters: 
 *      fh                 previously opened MPI file handle 
 *      buftype            MPI datatype <b>for</b> memory layout 
 *                         (Assumes a compatible view has been set on fh) 
 *      bufcount           # buftype elements to transfer 
 *------------------------------------------------------------------------*/ 
 
/* this macro switches which buffer "x" is pointing to */ 
<b>#define</b> TOGGLE_PTR(x) (((x)==(buffer1)) ? (x=buffer2) : (x=buffer1)) 
 
<b>void</b> double_buffer(<b>MPI_File</b> fh, <b>MPI_Datatype</b> buftype, <b>int</b> bufcount) 
{ 
 
   <b>MPI_Status</b> status;         /* status <b>for</b> MPI calls */ 
   <b>float</b> *buffer1, *buffer2;  /* buffers to hold results */ 
   <b>float</b> *compute_buf_ptr;    /* destination  buffer */ 
                              /*   <b>for</b> computing */ 
   <b>float</b> *write_buf_ptr;      /* source <b>for</b> writing */ 
   <b>int</b> done;                  /* determines when to quit */ 
 
   /* buffer initialization */ 
   buffer1 = (<b>float</b> *) malloc(bufcount*<b>sizeof</b>(<b>float</b>)); 
   buffer2 = (<b>float</b> *) malloc(bufcount*<b>sizeof</b>(<b>float</b>)); 
   compute_buf_ptr = buffer1;    /* initially point to buffer1 */ 
   write_buf_ptr   = buffer1;    /* initially point to buffer1 */ 
 
 
   /* DOUBLE-BUFFER prolog:  
    *   compute buffer1; then initiate writing buffer1 to disk  
    */ 
   compute_buffer(compute_buf_ptr, bufcount, &amp;done); 
   <b>MPI_File_write_all_begin</b>(fh, write_buf_ptr, bufcount, buftype); 
 
   /* DOUBLE-BUFFER steady state:  
    *  Overlap writing old results from buffer pointed to by write_buf_ptr  
    *  with computing new results into buffer pointed to by compute_buf_ptr. 
    * 
    *  There is always one write-buffer and one compute-buffer in use  
    *  during steady state. 
    */ 
   <b>while</b> (!done) { 
      TOGGLE_PTR(compute_buf_ptr); 
      compute_buffer(compute_buf_ptr, bufcount, &amp;done); 
      <b>MPI_File_write_all_end</b>(fh, write_buf_ptr, &amp;status); 
      TOGGLE_PTR(write_buf_ptr); 
      <b>MPI_File_write_all_begin</b>(fh, write_buf_ptr, bufcount, buftype); 
   } 
 
   /* DOUBLE-BUFFER epilog: 
    *   wait <b>for</b> final write to complete. 
    */ 
   <b>MPI_File_write_all_end</b>(fh, write_buf_ptr, &amp;status); 
 
 
   /* buffer cleanup */ 
   free(buffer1); 
   free(buffer2); 
} 
</tt></pre> 
  
  
<P> 

<P>
<hr>
<a href="node399.htm#Node399"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node399.htm#Node399"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node401.htm#Node401"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node399.htm#Node399"> Examples</a>
<b>Next: </b><a href="node401.htm#Node401"> Subarray Filetype Constructor</a>
<b>Previous: </b><a href="node399.htm#Node399"> Examples</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
