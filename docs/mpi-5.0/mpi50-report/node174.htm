<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-coll/coll-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Correctness</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h1><span id="Node174">7.14. Correctness</span></h1>
<a href="node173.htm#Node173"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node115.htm#Node115"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node175.htm#Node175"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node115.htm#Node115"> Collective Communication</a>
<b>Next: </b><a href="node175.htm#Node175"> Groups, Contexts, Communicators, and Caching</a>
<b>Previous: </b><a href="node173.htm#Node173"> Persistent Exclusive Scan</a>
<p>
  
  
<P> 
A correct, portable program must invoke collective communications so  
that deadlock will  
not occur, whether collective communications are synchronizing or not.  
The following examples illustrate dangerous use of collective routines  
on intra-communicators.  
<P> 
<br><b> Example</b>  
  
The following is erroneous.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt>/* ---------------- THIS EXAMPLE IS ERRONEOUS --------------- */ 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>MPI_Bcast</b>(buf2, count, type, 1, comm); 
        <b>break</b>; 
    <b>case</b> 1: 
        <b>MPI_Bcast</b>(buf2, count, type, 1, comm); 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>break</b>; 
} 
</tt></pre> 
  
<P> 
We assume that the group of <tt>comm</tt> is {0,1}.  
Two <font face="sans-serif"> MPI</font> processes execute two broadcast operations in reverse order.  If  
the operation is synchronizing then a deadlock will occur.  
<P> 
Collective  
operations must be executed in the same order at all members of the  
communication group.  
  
<P> 
<br><b> Example</b>  
  
The following is erroneous.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt>/* ---------------- THIS EXAMPLE IS ERRONEOUS --------------- */ 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm0); 
        <b>MPI_Bcast</b>(buf2, count, type, 2, comm2); 
        <b>break</b>; 
    <b>case</b> 1: 
        <b>MPI_Bcast</b>(buf1, count, type, 1, comm1); 
        <b>MPI_Bcast</b>(buf2, count, type, 0, comm0); 
        <b>break</b>; 
    <b>case</b> 2: 
        <b>MPI_Bcast</b>(buf1, count, type, 2, comm2); 
        <b>MPI_Bcast</b>(buf2, count, type, 1, comm1); 
        <b>break</b>; 
} 
</tt></pre> 
  
<P> 
Assume that the group of  
<tt>comm0</tt> is {0,1}, of <tt>comm1</tt> is {1,2} and of <tt>comm2</tt>  
is {2,0}.  If the broadcast is a synchronizing operation, then there  
is a cyclic dependency: the broadcast in <tt>comm2</tt> completes only  
after the broadcast in <tt>comm0</tt>; the broadcast in <tt>comm0</tt>  
completes only after the broadcast in <tt>comm1</tt>; and the broadcast  
in <tt>comm1</tt> completes only after the broadcast in <tt>comm2</tt>.  
Thus, the code will deadlock.  
<P> 
Collective operations must be executed in an order so that  
no cyclic dependencies occur. Nonblocking collective operations can  
alleviate this issue.  
  
<P> 
<br><b> Example</b>  
  
The following is erroneous.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt>/* ---------------- THIS EXAMPLE IS ERRONEOUS --------------- */ 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>MPI_Send</b>(buf2, count, type, 1, tag, comm); 
        <b>break</b>; 
    <b>case</b> 1: 
        <b>MPI_Recv</b>(buf2, count, type, 0, tag, comm, status); 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>break</b>; 
} 
</tt></pre> 
  
<P> 
<font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 0</font> executes a broadcast, followed by a blocking send operation.  
<font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 1</font> first executes a blocking receive that matches the send,  
followed by a broadcast call that matches the broadcast of <font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 0</font>.  
This program may deadlock.  The broadcast call on <font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 0</font>  
<em> may</em> block until <font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 1</font> executes the matching  
broadcast call, so that the  
send is not executed.  <font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 1</font> will definitely block on the  
receive and so, in this case, never executes the  
broadcast.  
<P> 
The relative order of execution of collective operations and point-to-point  
operations must be such, so that even if the collective  
operations and the point-to-point operations are synchronizing, no  
deadlock will occur.  
  
<P> 
<br><b> Example</b>  
  
An unsafe, nondeterministic program.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>switch</b>(rank) { 
    <b>case</b> 0: 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>MPI_Send</b>(buf2, count, type, 1, tag, comm); 
        <b>break</b>; 
    <b>case</b> 1: 
        <b>MPI_Recv</b>(buf2, count, type, <b>MPI_ANY_SOURCE</b>, tag, comm, status); 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>MPI_Recv</b>(buf2, count, type, <b>MPI_ANY_SOURCE</b>, tag, comm, status); 
        <b>break</b>; 
    <b>case</b> 2: 
        <b>MPI_Send</b>(buf2, count, type, 1, tag, comm); 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>break</b>; 
} 
</tt></pre> 
  
<P> 
All three <font face="sans-serif"> MPI</font> processes participate in a broadcast.  <font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 0</font> sends a message to  
<font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 1</font> after the broadcast, and <font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 2</font> sends a message  
to <font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 1</font> before  
the broadcast.  <font face="sans-serif"> MPI</font> process with rank <font face="sans-serif"> 1</font> receives before and after the broadcast, with a  
wildcard source argument.  
<P> 
Two possible executions of this program, with different matchings  
of sends and receives, are  
illustrated in Figure <a href="node174.htm#Figure15">15</a>.  
Note that the second execution has the peculiar effect that a send executed  
after the broadcast is received at another node before the broadcast.  
This example illustrates the fact that one should not rely on  
collective communication functions to have particular synchronization  
effects.  
A program that works correctly only when the first execution occurs  
(only when broadcast is synchronizing) is erroneous.  
  
<P> 
  <div style="text-align:center"><P><img width=783 height=805 src="coll-matchings.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 15: </b><span id="Figure15">A race condition causes nondeterministic matching of sends 
    and receives.  One cannot rely on synchronization from a broadcast 
    to make the program deterministic.</span><P> 
  
    
Finally, in multithreaded implementations, one can have more than one,  
concurrently executing, collective communication initialization call at an <font face="sans-serif"> MPI</font> process.  
In these situations, it is the user's responsibility to ensure that  
the same communicator is not used concurrently by two different  
collective communication initialization calls at the same <font face="sans-serif"> MPI</font> process.  
Collective communication initialization calls include  
all calls for blocking collective operations,  
all initiation calls for nonblocking collective operations, and  
all initialization calls for persistent collective operations.  
<P> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
Assume that broadcast is implemented using point-to-point <font face="sans-serif"> MPI</font> communication.  
Suppose the following two rules are followed.  
<ol> 
 
1. All receives specify their source explicitly (no wildcards).  
 
<br> 
2. Each <font face="sans-serif"> MPI</font> process sends all messages that pertain to one collective call before  
sending any message that pertain to a subsequent collective call.  
</ol> 
Then, messages belonging to successive broadcasts cannot be confused,  
as the order of point-to-point messages is preserved.  
<P> 
It is the implementor's responsibility to  
ensure that point-to-point messages are not confused with collective  
messages.  One way to accomplish this is, whenever a communicator is  
created, to also create a ``hidden communicator'' for collective communication.  
One could achieve a similar  
effect more cheaply, for example, by using a hidden  
tag or context bit to indicate  
whether the communicator is used for point-to-point or collective  
communication.  
 (<em> End of advice to implementors.</em>) <br> 
<br><b> Example</b>  
  
Blocking and nonblocking collective operations can be interleaved, i.e.,  
a blocking collective operation can be posted even if there is a  
nonblocking collective operation outstanding.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Request</b> req; 
 
<b>MPI_Ibarrier</b>(comm, &amp;req); 
<b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
<b>MPI_Wait</b>(&amp;req, <b>MPI_STATUS_IGNORE</b>); 
</tt></pre> 
  
<P> 
Each <font face="sans-serif"> MPI</font> process starts a nonblocking barrier operation, participates in a  
blocking broadcast and then waits until every other <font face="sans-serif"> MPI</font> process started the  
barrier operation. This effectively turns the broadcast into a  
synchronizing broadcast with possible communication/communication  
overlap (<font face="sans-serif"> MPI_Bcast</font> is allowed, but not required to  
synchronize).  
  
<P> 
<br><b> Example</b>  
  
The starting order of collective operations on a particular communicator  
defines their matching. The following example shows an erroneous  
matching of different collective operations on the same communicator.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt>/* ---------------- THIS EXAMPLE IS ERRONEOUS --------------- */ 
<b>MPI_Request</b> req; 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
        /* erroneous matching */ 
        <b>MPI_Ibarrier</b>(comm, &amp;req); 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>MPI_Wait</b>(&amp;req, <b>MPI_STATUS_IGNORE</b>); 
        <b>break</b>; 
    <b>case</b> 1: 
        /* erroneous matching */ 
        <b>MPI_Bcast</b>(buf1, count, type, 0, comm); 
        <b>MPI_Ibarrier</b>(comm, &amp;req); 
        <b>MPI_Wait</b>(&amp;req, <b>MPI_STATUS_IGNORE</b>); 
        <b>break</b>; 
} 
</tt></pre> 
  
<P> 
This ordering would match <font face="sans-serif"> MPI_Ibarrier</font> on rank 0 with  
<font face="sans-serif"> MPI_Bcast</font> on rank 1, which is erroneous and the program behavior is  
undefined. However, if such an order is required, the user must create  
different duplicate communicators and perform the operations on them.  
If started with two <font face="sans-serif"> MPI</font> processes, the following program would be correct:  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Request</b> req; 
<b>MPI_Comm</b> dupcomm; 
<b>MPI_Comm_dup</b>(comm, &amp;dupcomm); 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
        <b>MPI_Ibarrier</b>(comm, &amp;req); 
        <b>MPI_Bcast</b>(buf1, count, type, 0, dupcomm); 
        <b>MPI_Wait</b>(&amp;req, <b>MPI_STATUS_IGNORE</b>); 
        <b>break</b>; 
    <b>case</b> 1: 
        <b>MPI_Bcast</b>(buf1, count, type, 0, dupcomm); 
        <b>MPI_Ibarrier</b>(comm, &amp;req); 
        <b>MPI_Wait</b>(&amp;req, <b>MPI_STATUS_IGNORE</b>); 
        <b>break</b>; 
} 
</tt></pre> 
  
<P> 
  
<P> 
 
<br> 
<em> Advice to users.</em>  
<P> 
The use of different communicators offers some flexibility regarding the  
matching of nonblocking collective operations. In this sense,  
communicators could be used as an equivalent to tags. However,  
communicator construction might induce overheads so that this should be  
used carefully.  
 (<em> End of advice to users.</em>) <br> 
<br><b> Example</b>  
  
Nonblocking collective operations can rely on the similar progress rules  
as nonblocking point-to-point operations. Thus, if started with two  
<font face="sans-serif"> MPI</font> processes, the following program is a valid <font face="sans-serif"> MPI</font> program and is  
guaranteed to terminate:  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Request</b> req; 
 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
      <b>MPI_Ibarrier</b>(comm, &amp;req); 
      <b>MPI_Wait</b>(&amp;req, <b>MPI_STATUS_IGNORE</b>); 
      <b>MPI_Send</b>(buf, count, dtype, 1, tag, comm); 
      <b>break</b>; 
    <b>case</b> 1: 
      <b>MPI_Ibarrier</b>(comm, &amp;req); 
      <b>MPI_Recv</b>(buf, count, dtype, 0, tag, comm, <b>MPI_STATUS_IGNORE</b>); 
      <b>MPI_Wait</b>(&amp;req, <b>MPI_STATUS_IGNORE</b>); 
      <b>break</b>; 
} 
</tt></pre> 
  
<P> 
The <font face="sans-serif"> MPI</font> library must <em> progress</em>  
the barrier in the <font face="sans-serif"> MPI_Recv</font>  
call. Thus, the <font face="sans-serif"> MPI_Wait</font> call in rank 0 will eventually  
complete, which enables the matching <font face="sans-serif"> MPI_Send</font> so all calls  
eventually return.  
<P> 
  
<P> 
<br><b> Example</b>  
  
  
Blocking and nonblocking collective operations do not match. The  
following example is erroneous.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt>/* ---------------- THIS EXAMPLE IS ERRONEOUS --------------- */ 
<b>MPI_Request</b> req; 
 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
      /* erroneous false matching of Alltoall and Ialltoall */ 
      <b>MPI_Ialltoall</b>(sbuf, scnt, stype, rbuf, rcnt, rtype, comm, &amp;req); 
      <b>MPI_Wait</b>(&amp;req, <b>MPI_STATUS_IGNORE</b>); 
      <b>break</b>; 
    <b>case</b> 1: 
      /* erroneous false matching of Alltoall and Ialltoall */ 
      <b>MPI_Alltoall</b>(sbuf, scnt, stype, rbuf, rcnt, rtype, comm); 
      <b>break</b>; 
} 
</tt></pre> 
  
  
<P> 
<br><b> Example</b>  
  
Collective and point-to-point requests can be mixed in functions that  
enable multiple completions. If started with two <font face="sans-serif"> MPI</font> processes, the  
following program is valid.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Request</b> reqs[2]; 
 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
      <b>MPI_Ibarrier</b>(comm, &amp;reqs[0]); 
      <b>MPI_Send</b>(buf, count, dtype, 1, tag, comm); 
      <b>MPI_Wait</b>(&amp;reqs[0], <b>MPI_STATUS_IGNORE</b>); 
      <b>break</b>; 
    <b>case</b> 1: 
      <b>MPI_Irecv</b>(buf, count, dtype, 0, tag, comm, &amp;reqs[0]); 
      <b>MPI_Ibarrier</b>(comm, &amp;reqs[1]); 
      <b>MPI_Waitall</b>(2, reqs, <b>MPI_STATUSES_IGNORE</b>); 
      <b>break</b>; 
} 
</tt></pre> 
  
<P> 
The <font face="sans-serif"> MPI_Waitall</font> call returns only after the barrier and the receive completed.  
<P> 
  
<P> 
<br><b> Example</b>  
  
Multiple nonblocking collective operations can be outstanding on a  
single communicator and match in order.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Request</b> reqs[3]; 
 
compute(buf1); 
<b>MPI_Ibcast</b>(buf1, count, type, 0, comm, &amp;reqs[0]); 
compute(buf2); 
<b>MPI_Ibcast</b>(buf2, count, type, 0, comm, &amp;reqs[1]); 
compute(buf3); 
<b>MPI_Ibcast</b>(buf3, count, type, 0, comm, &amp;reqs[2]); 
<b>MPI_Waitall</b>(3, reqs, <b>MPI_STATUSES_IGNORE</b>); 
</tt></pre> 
  
<P> 
  
<P> 
 
<br> 
<em> Advice to users.</em>  
<P> 
Pipelining and double-buffering techniques can efficiently be used to  
overlap computation and communication. However, having too many  
outstanding requests might have a negative impact on performance.  
 (<em> End of advice to users.</em>) <br> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
The use of pipelining may generate many outstanding requests. A  
high-quality hardware-supported implementation with limited resources  
should be able to fall back to a software implementation if its  
resources are exhausted. In this way, the implementation could limit the  
number of outstanding requests only by the available memory.  
 (<em> End of advice to implementors.</em>) <br> 
<br><b> Example</b>  
  
Nonblocking collective operations can also be used to enable  
simultaneous collective operations on multiple overlapping  
communicators (see Figure <a href="node174.htm#Figure16">16</a>). The following example is started with three <font face="sans-serif"> MPI</font> processes and  
three communicators. The first communicator <tt>comm1</tt> includes ranks 0 and  
1, <tt>comm2</tt> includes ranks 1 and 2, and <tt>comm3</tt> spans ranks 0 and 2. It is not  
possible to perform a blocking collective operation on all communicators  
because there exists no deadlock-free order to invoke them. However,  
nonblocking collective operations can easily be used to achieve this  
task.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Request</b> reqs[2]; 
 
<b>switch</b>(rank) { 
    <b>case</b> 0: 
      <b>MPI_Iallreduce</b>(sbuf1, rbuf1, count, dtype, <b>MPI_SUM</b>, comm1, &amp;reqs[0]); 
      <b>MPI_Iallreduce</b>(sbuf3, rbuf3, count, dtype, <b>MPI_SUM</b>, comm3, &amp;reqs[1]); 
      <b>break</b>; 
    <b>case</b> 1: 
      <b>MPI_Iallreduce</b>(sbuf1, rbuf1, count, dtype, <b>MPI_SUM</b>, comm1, &amp;reqs[0]); 
      <b>MPI_Iallreduce</b>(sbuf2, rbuf2, count, dtype, <b>MPI_SUM</b>, comm2, &amp;reqs[1]); 
      <b>break</b>; 
    <b>case</b> 2: 
      <b>MPI_Iallreduce</b>(sbuf2, rbuf2, count, dtype, <b>MPI_SUM</b>, comm2, &amp;reqs[0]); 
      <b>MPI_Iallreduce</b>(sbuf3, rbuf3, count, dtype, <b>MPI_SUM</b>, comm3, &amp;reqs[1]); 
      <b>break</b>; 
} 
<b>MPI_Waitall</b>(2, reqs, <b>MPI_STATUSES_IGNORE</b>); 
</tt></pre> 
  
<P> 
  
<P> 
  <div style="text-align:center"><P><img width=360 height=262 src="overlap_comms.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 16: </b><span id="Figure16">Example with overlapping 
  communicators</span><P> 
  
    
 
<br> 
<em> Advice to users.</em>  
<P> 
This method can be useful if overlapping neighboring regions (halo  
or ghost zones) are used in collective operations. The sequence of the  
two calls in each <font face="sans-serif"> MPI</font> process is irrelevant because the two nonblocking  
operations are performed on different communicators.  
 (<em> End of advice to users.</em>) <br> 
<br><b> Example</b>  
  
The <em> progress</em>  
of multiple outstanding nonblocking collective operations  
is completely independent.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>MPI_Request</b> reqs[2]; 
 
compute(buf1); 
<b>MPI_Ibcast</b>(buf1, count, type, 0, comm, &amp;reqs[0]); 
compute(buf2); 
<b>MPI_Ibcast</b>(buf2, count, type, 0, comm, &amp;reqs[1]); 
<b>MPI_Wait</b>(&amp;reqs[1], <b>MPI_STATUS_IGNORE</b>); 
/* nothing is known about the status of the first bcast here */ 
<b>MPI_Wait</b>(&amp;reqs[0], <b>MPI_STATUS_IGNORE</b>); 
</tt></pre> 
  
<P> 
Finishing the second <font face="sans-serif"> MPI_IBCAST</font> is completely independent of  
the first one. This means that it is not guaranteed that the first  
broadcast operation is finished or even started after the second one is  
completed via <tt>reqs[1]</tt>.  
  
<P> 
<P> 
  

<P>
<hr>
<a href="node173.htm#Node173"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node115.htm#Node115"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node175.htm#Node175"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node115.htm#Node115"> Collective Communication</a>
<b>Next: </b><a href="node175.htm#Node175"> Groups, Contexts, Communicators, and Caching</a>
<b>Previous: </b><a href="node173.htm#Node173"> Persistent Exclusive Scan</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
