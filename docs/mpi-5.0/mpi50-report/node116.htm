<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-coll/coll-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Introduction and Overview</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h1><span id="Node116">7.1. Introduction and Overview</span></h1>
<a href="node115.htm#Node115"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node115.htm#Node115"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node117.htm#Node117"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node115.htm#Node115"> Collective Communication</a>
<b>Next: </b><a href="node117.htm#Node117"> Communicator Argument</a>
<b>Previous: </b><a href="node115.htm#Node115"> Collective Communication</a>
<p>
  
<P> 
Collective communication is defined as communication that involves  
a group  
or groups  
of <font face="sans-serif"> MPI</font> processes.  
The functions of this type provided by <font face="sans-serif"> MPI</font> are the following:  
<ul> 
 
<li><font face="sans-serif"> MPI_BARRIER</font>, <font face="sans-serif"> MPI_IBARRIER</font>, <font face="sans-serif"> MPI_BARRIER_INIT</font>:  
Barrier synchronization across  
all members of a group  
(Section <a href="node121.htm#Node121">Barrier Synchronization</a>, Section <a href="node148.htm#Node148">Nonblocking Barrier Synchronization</a>, and Section <a href="node162.htm#Node162">Persistent Barrier Synchronization</a>).  
 
<li><font face="sans-serif"> MPI_BCAST</font>, <font face="sans-serif"> MPI_IBCAST</font>, <font face="sans-serif"> MPI_BCAST_INIT</font>:  
Broadcast from one member to all members of a group  
(Section <a href="node122.htm#Node122">Broadcast</a>, Section <a href="node149.htm#Node149">Nonblocking Broadcast</a>, and Section <a href="node163.htm#Node163">Persistent Broadcast</a>).  
This is shown  
as ``broadcast''  
in Figure <a href="node116.htm#Figure4">4</a>.  
 
<li><font face="sans-serif"> MPI_GATHER</font>, <font face="sans-serif"> MPI_IGATHER</font>, <font face="sans-serif"> MPI_GATHER_INIT</font>,  
<font face="sans-serif"> MPI_GATHERV</font>, <font face="sans-serif"> MPI_IGATHERV</font>, <font face="sans-serif"> MPI_GATHERV_INIT</font>,  
:  
Gather data from  
all members of a group  
to one member  
(Section <a href="node124.htm#Node124">Gather</a>, Section <a href="node151.htm#Node151">Nonblocking Gather</a>, and Section <a href="node164.htm#Node164">Persistent Gather</a>).  
This is shown  
as ``gather''  
in Figure <a href="node116.htm#Figure4">4</a>.  
 
<li><font face="sans-serif"> MPI_SCATTER</font>, <font face="sans-serif"> MPI_ISCATTER</font>, <font face="sans-serif"> MPI_SCATTER_INIT</font>,  
<font face="sans-serif"> MPI_SCATTERV</font>, <font face="sans-serif"> MPI_ISCATTERV</font>, <font face="sans-serif"> MPI_SCATTERV_INIT</font>:  
Scatter data from one member to all members of a group  
(Section <a href="node126.htm#Node126">Scatter</a>, Section <a href="node152.htm#Node152">Nonblocking Scatter</a>, and Section <a href="node165.htm#Node165">Persistent Scatter</a>).  
This is shown  
as ``scatter''  
in Figure <a href="node116.htm#Figure4">4</a>.  
 
<li><font face="sans-serif"> MPI_ALLGATHER</font>, <font face="sans-serif"> MPI_IALLGATHER</font>, <font face="sans-serif"> MPI_ALLGATHER_INIT</font>,  
<font face="sans-serif"> MPI_ALLGATHERV</font>, <font face="sans-serif"> MPI_IALLGATHERV</font>, <font face="sans-serif"> MPI_ALLGATHERV_INIT</font>:  
A variation on Gather where all members of  
a  
group receive the result  
(Section <a href="node128.htm#Node128">All-Gather</a>, Section <a href="node153.htm#Node153">Nonblocking All-Gather</a>, and Section <a href="node166.htm#Node166">Persistent All-Gather</a>).  
This is shown as ``allgather'' in Figure <a href="node116.htm#Figure4">4</a>.  
 
<li><font face="sans-serif"> MPI_ALLTOALL</font>, <font face="sans-serif"> MPI_IALLTOALL</font>, <font face="sans-serif"> MPI_ALLTOALL_INIT</font>,  
<font face="sans-serif"> MPI_ALLTOALLV</font>, <font face="sans-serif"> MPI_IALLTOALLV</font>, <font face="sans-serif"> MPI_ALLTOALLV_INIT</font>,  
<font face="sans-serif"> MPI_ALLTOALLW</font>, <font face="sans-serif"> MPI_IALLTOALLW</font>, <font face="sans-serif"> MPI_ALLTOALLW_INIT</font>:  
Scatter/Gather data from all members to all members of a group  
(also called complete exchange)  
(Section <a href="node130.htm#Node130">All-to-All Scatter/Gather</a>, Section <a href="node154.htm#Node154">Nonblocking All-to-All Scatter/Gather</a>, and Section <a href="node167.htm#Node167">Persistent All-to-All Scatter/Gather</a>).  
This is shown as ``complete exchange'' in Figure <a href="node116.htm#Figure4">4</a>.  
 
<li><font face="sans-serif"> MPI_ALLREDUCE</font>, <font face="sans-serif"> MPI_IALLREDUCE</font>, <font face="sans-serif"> MPI_ALLREDUCE_INIT</font>,  
<font face="sans-serif"> MPI_REDUCE</font>, <font face="sans-serif"> MPI_IREDUCE</font>, <font face="sans-serif"> MPI_REDUCE_INIT</font>:  
Global reduction operations such as sum, max, min, or user-defined functions,  
where the result  
is returned to  
all members of a  
group (Section <a href="node138.htm#Node138">All-Reduce</a>, Section <a href="node156.htm#Node156">Nonblocking All-Reduce</a>, and Section <a href="node169.htm#Node169">Persistent All-Reduce</a>)  
and a variation where the result is  
returned to only one member  
(Section <a href="node131.htm#Node131">Global Reduction Operations</a>, Section <a href="node155.htm#Node155">Nonblocking Reduce</a>, and Section <a href="node168.htm#Node168">Persistent Reduce</a>).  
 
<li><font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK</font>, <font face="sans-serif"> MPI_IREDUCE_SCATTER_BLOCK</font>, <font face="sans-serif"> MPI_REDUCE_SCATTER_BLOCK_INIT</font>, <font face="sans-serif"> MPI_REDUCE_SCATTER</font>, <font face="sans-serif"> MPI_IREDUCE_SCATTER</font>, <font face="sans-serif"> MPI_REDUCE_SCATTER_INIT</font>:  
A combined reduction and scatter operation  
(Section <a href="node140.htm#Node140">Reduce-Scatter</a>,  
Section <a href="node157.htm#Node157">Nonblocking Reduce-Scatter with Equal Blocks</a>, Section <a href="node158.htm#Node158">Nonblocking Reduce-Scatter</a>, Section <a href="node170.htm#Node170">Persistent Reduce-Scatter with Equal Blocks</a>, and Section <a href="node171.htm#Node171">Persistent Reduce-Scatter</a>).  
 
<li><font face="sans-serif"> MPI_SCAN</font>, <font face="sans-serif"> MPI_ISCAN</font>, <font face="sans-serif"> MPI_SCAN_INIT</font>,  
<font face="sans-serif"> MPI_EXSCAN</font>, <font face="sans-serif"> MPI_IEXSCAN</font>, <font face="sans-serif"> MPI_EXSCAN_INIT</font>:  
Scan across all members of a group (also called prefix)  
(Section <a href="node143.htm#Node143">Scan</a>,  
Section <a href="node145.htm#Node145">Exclusive Scan</a>, Section <a href="node159.htm#Node159">Nonblocking Inclusive Scan</a>,  
Section <a href="node160.htm#Node160">Nonblocking Exclusive Scan</a>, Section <a href="node172.htm#Node172">Persistent Inclusive Scan</a>, and Section <a href="node173.htm#Node173">Persistent Exclusive Scan</a>).  
</ul> 
<br> 
  <div style="text-align:center"><P><img width=733 height=1201 src="coll-fig1-22.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 4: </b><span id="Figure4">Collective move functions illustrated 
  for a group of six <font face="sans-serif"> MPI</font> processes. In each case, each row of boxes 
  represents data locations in one <font face="sans-serif"> MPI</font> process. Thus, in the broadcast, 
  initially just the first <font face="sans-serif"> MPI</font> process contains the data $A_0$, but after the 
  broadcast all <font face="sans-serif"> MPI</font> processes contain it.</span><P> 
  
    
One of the key arguments  
in a call to a collective routine  
is a communicator that defines the group  
or groups  
of participating <font face="sans-serif"> MPI</font> processes and provides a context for the operation.  
This is discussed further in Section <a href="node117.htm#Node117">Communicator Argument</a>.  
The syntax and semantics of the collective operations are  
defined to be consistent with the syntax and semantics of the  
point-to-point operations. Thus, general datatypes are allowed  
and must match between sending and receiving <font face="sans-serif"> MPI</font> processes as specified  
in  
Chapter <a href="node97.htm#Node97">Datatypes</a>.  
Several collective routines such as broadcast and gather have  
a single originating or receiving <font face="sans-serif"> MPI</font> process.  
Such an <font face="sans-serif"> MPI</font> process is  
called the <b> root</b>.  
Some arguments in the collective functions are specified as  
``significant only at root,'' and are ignored for all  
participants except the root.  
<P> 
 
<br> 
<em> Advice to users.</em>  
<P> 
Note that the programmer is still responsible for avoiding undefined behavior in the host language by not passing uninitialized values to <font face="sans-serif"> MPI</font> procedure calls.  
 (<em> End of advice to users.</em>) <br> 
The reader is referred to Chapter <a href="node97.htm#Node97">Datatypes</a>  
for information concerning communication buffers,  
general datatypes and type matching rules, and to  
Chapter <a href="node175.htm#Node175">Groups, Contexts, Communicators, and Caching</a> for information on how to define groups and  
create communicators.  
<P> 
The type-matching conditions for the collective operations are more  
strict than the corresponding conditions between sender and receiver  
in point-to-point.  Namely, for collective operations,  
the amount of data sent must exactly  
match the amount of data specified by the receiver.  
Different  
type maps (the layout in memory, see Section <a href="node98.htm#Node98">Derived Datatypes</a>)  
between sender and receiver are still allowed.  
<P> 
Collective operations can (but are not required to)  
complete as soon as the  
caller's  
participation in the collective communication is  
finished.  A blocking operation is  
complete as soon as the call returns. A nonblocking (immediate) call  
requires a separate completion call (cf. Section <a href="node73.htm#Node73">Nonblocking Communication</a>).  
The completion  
of a collective operation indicates that the caller  
is free to modify locations in the  
communication buffer.  It does not indicate that other <font face="sans-serif"> MPI</font> processes in  
the group have completed or even  
started the operation (unless otherwise  
implied by  
the description of the operation).  
Thus, a collective communication operation may, or may not,  
have the effect of synchronizing all participating <font face="sans-serif"> MPI</font> processes.  
<P> 
Collective communication calls may use the same  
communicators as point-to-point communication; <font face="sans-serif"> MPI</font> guarantees that  
messages generated on behalf of collective communication calls will not  
be confused with messages generated by point-to-point communication.  
The collective operations do not have a message tag argument.  
A more detailed discussion of correct use of collective  
routines is found in Section <a href="node174.htm#Node174">Correctness</a>.  
<P> 
 
<br> 
<em> Rationale.</em>  
<P> 
The equal-data restriction (on type matching) was made so  
as to avoid the complexity  
of providing a facility analogous to the status  
argument of <font face="sans-serif"> MPI_RECV</font> for discovering the amount of data sent.  
Some of the collective routines would require an array of status values.  
<P> 
The statements about synchronization are made so as to allow a variety  
of implementations of the collective functions.  
<P> 
 (<em> End of rationale.</em>) <br> 
 
<br> 
<em> Advice to users.</em>  
<P> 
It is dangerous to rely on synchronization  
side-effects of the collective operations for program correctness.  
For example, even though a particular implementation may  
provide a broadcast routine  
with a side-effect of synchronization, the standard does not require  
this, and a program that relies on this will not be portable.  
<P> 
On the other hand, a correct, portable program must allow for the fact  
that a collective call <em> may</em> be synchronizing.  Though one cannot  
rely on any synchronization side-effect, one must program so as to allow  
it.  These issues are discussed further in Section <a href="node174.htm#Node174">Correctness</a>.  
 (<em> End of advice to users.</em>) <br> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
While vendors may write optimized collective routines matched to  
their architectures, a complete library of the collective communication  
routines can be written entirely using the <font face="sans-serif"> MPI</font> point-to-point communication  
functions and a few auxiliary functions.  If implementing on top of  
point-to-point, a hidden, special communicator might  
be created for the  
collective operation so as to avoid interference with any on-going  
point-to-point communication at the time of the collective call.  This  
is discussed further in Section <a href="node174.htm#Node174">Correctness</a>.  
 (<em> End of advice to implementors.</em>) <br> 
Many of the descriptions of the collective routines provide illustrations in  
terms of blocking <font face="sans-serif"> MPI</font> point-to-point routines.  These are intended solely to  
indicate what data is sent or received by which <font face="sans-serif"> MPI</font> process.  Many of these  
examples  
are <em> not</em> correct <font face="sans-serif"> MPI</font> programs; for purposes of simplicity, they often  
assume infinite buffering.  
<P> 

<P>
<hr>
<a href="node115.htm#Node115"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node115.htm#Node115"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node117.htm#Node117"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node115.htm#Node115"> Collective Communication</a>
<b>Next: </b><a href="node117.htm#Node117"> Communicator Argument</a>
<b>Previous: </b><a href="node115.htm#Node115"> Collective Communication</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
