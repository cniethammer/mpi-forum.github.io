<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-one-side/one-side-2-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi5-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi50-report-html.idx --lstlisting -basedef mpi5defs.txt -o mpi50-report.tex mpi-reporthtml.tex 
-->
<title>Examples</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h1><span id="Node344">13.8. Examples</span></h1>
<a href="node343.htm#Node343"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node308.htm#Node308"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node345.htm#Node345"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node308.htm#Node308"> One-Sided Communications</a>
<b>Next: </b><a href="node345.htm#Node345"> External Interfaces</a>
<b>Previous: </b><a href="node343.htm#Node343"> Registers and Compiler Optimizations</a>
<p>
  
<P> 
<br><b> Example</b>  
  
The following example shows a generic loosely synchronous, iterative  
code, using <font face="sans-serif"> MPI_WIN_FENCE</font> for synchronization.  The window at each <font face="sans-serif"> MPI</font> process  
consists of array <tt>A</tt>, which contains the origin and target buffers of  
the  
put operations.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt>... 
<b>while</b> (!converged(A)) { 
  update(A); 
  <b>MPI_Win_fence</b>(<b>MPI_MODE_NOPRECEDE</b>, win); 
  <b>for</b>(i=0; i &lt; toneighbors; i++) 
    <b>MPI_Put</b>(&amp;frombuf[i], 1, fromtype[i], toneighbor[i], 
            todisp[i], 1, totype[i], win); 
  <b>MPI_Win_fence</b>((<b>MPI_MODE_NOSTORE</b> | <b>MPI_MODE_NOSUCCEED</b>), win); 
} 
</tt></pre> 
  
The same code could be written with get rather than put.  Note that,  
during the communication phase, each  
window is concurrently read  (as origin buffer of puts) and written  
(as target buffer of puts).  This is OK, provided that there is no  
overlap between the target buffer of a put and another communication   
buffer.  
  
<P> 
<br><b> Example</b>  
  
Same generic example, with more computation/communication overlap.  We  
assume that the update phase is broken into two  
subphases: the first,   
where the ``boundary,'' which is involved in communication, is updated, and  
the second, where the ``core,'' which neither  
uses nor provides   
communicated data, is updated.  
<br> 
<pre style="background-color:#EFEFEF"><tt>... 
<b>while</b> (!converged(A)) { 
  update_boundary(A); 
  <b>MPI_Win_fence</b>((<b>MPI_MODE_NOPUT</b> | <b>MPI_MODE_NOPRECEDE</b>), win); 
  <b>for</b>(i=0; i &lt; fromneighbors; i++) 
    <b>MPI_Get</b>(&amp;tobuf[i], 1, totype[i], fromneighbor[i], 
            fromdisp[i], 1, fromtype[i], win); 
  update_core(A); 
  <b>MPI_Win_fence</b>(<b>MPI_MODE_NOSUCCEED</b>, win); 
} 
</tt></pre> 
  
The get communication can be concurrent with the core update, since  
they do not access the same locations, and the local update of the  
origin buffer by the get operation can be concurrent with the local update  
of the core by the <tt>update_core</tt> call.  In order to get similar  
overlap with put communication we would need to use separate windows  
for the core and for the boundary.  
This is required   
because we do not allow local stores to be concurrent with puts  
on the same, or on overlapping, windows.  
  
<P> 
<br><b> Example</b>  
Same code as in Example <a href="node344.htm#Node344">Examples</a>,  
rewritten using post-start-complete-wait.  
<br> 
<pre style="background-color:#EFEFEF"><tt>... 
<b>while</b> (!converged(A)) { 
  update(A); 
  <b>MPI_Win_post</b>(fromgroup, 0, win); 
  <b>MPI_Win_start</b>(togroup, 0, win); 
  <b>for</b>(i=0; i &lt; toneighbors; i++) 
    <b>MPI_Put</b>(&amp;frombuf[i], 1, fromtype[i], toneighbor[i], 
            todisp[i], 1, totype[i], win); 
  <b>MPI_Win_complete</b>(win); 
  <b>MPI_Win_wait</b>(win); 
} 
</tt></pre> 
  
  
<P> 
<br><b> Example</b>  
Same example, with post-start-complete-wait, as in Example <a href="node344.htm#Node344">Examples</a>.  
<br> 
<pre style="background-color:#EFEFEF"><tt>... 
<b>while</b> (!converged(A)) { 
  update_boundary(A); 
  <b>MPI_Win_post</b>(togroup, <b>MPI_MODE_NOPUT</b>, win); 
  <b>MPI_Win_start</b>(fromgroup, 0, win); 
  <b>for</b>(i=0; i &lt; fromneighbors; i++) 
    <b>MPI_Get</b>(&amp;tobuf[i], 1, totype[i], fromneighbor[i], 
            fromdisp[i], 1, fromtype[i], win); 
  update_core(A); 
  <b>MPI_Win_complete</b>(win); 
  <b>MPI_Win_wait</b>(win); 
} 
</tt></pre> 
  
  
<P> 
<br><b> Example</b>  
[Double buffer in <font face="sans-serif"> RMA</font>]CDouble buffer in RMA@Double buffer in <font face="sans-serif"> RMA</font>MPI_Barrier,MPI_Win_post,MPI_Win_start,MPI_Get,MPI_Win_complete,MPI_Win_waitA checkerboard, or double buffer  communication pattern, that allows  
more computation/communication overlap.  Array <tt>A0</tt> is updated  
using values of array <tt>A1</tt>, and vice versa.  We assume that communication is symmetric: if process A gets data from process B, then process B gets data from process A.  Window <tt>wini</tt> consists of array <tt>Ai</tt>.  
<br> 
<pre style="background-color:#EFEFEF"><tt>... 
<b>if</b> (!converged(A0,A1)) 
  <b>MPI_Win_post</b>(neighbors, (<b>MPI_MODE_NOCHECK</b> | <b>MPI_MODE_NOPUT</b>), win0); 
<b>MPI_Barrier</b>(comm0); 
/* the barrier is needed because the start call inside the 
loop uses the nocheck option */ 
<b>while</b> (!converged(A0, A1)) { 
  /* communication on A0 and computation on A1 */ 
  update2(A1, A0); /* local update of A1 that depends on A0 (and A1) */ 
  <b>MPI_Win_start</b>(neighbors, <b>MPI_MODE_NOCHECK</b>, win0); 
  <b>for</b>(i=0; i &lt; fromneighbors; i++) 
    <b>MPI_Get</b>(&amp;tobuf0[i], 1, totype0[i], neighbor[i], 
            fromdisp0[i], 1, fromtype0[i], win0); 
  update1(A1); /* local update of A1 that is 
                  concurrent with communication that updates A0 */  
  <b>MPI_Win_post</b>(neighbors, (<b>MPI_MODE_NOCHECK</b> | <b>MPI_MODE_NOPUT</b>), win1); 
  <b>MPI_Win_complete</b>(win0); 
  <b>MPI_Win_wait</b>(win0); 
 
  /* communication on A1 and computation on A0 */ 
  update2(A0, A1); /* local update of A0 that depends on A1 (and A0) */ 
  <b>MPI_Win_start</b>(neighbors, <b>MPI_MODE_NOCHECK</b>, win1); 
  <b>for</b>(i=0; i &lt; fromneighbors; i++) 
    <b>MPI_Get</b>(&amp;tobuf1[i], 1, totype1[i], neighbor[i], 
            fromdisp1[i], 1, fromtype1[i], win1); 
  update1(A0); /* local update of A0 that depends on A0 only, 
                 concurrent with communication that updates A1 */ 
  <b>if</b> (!converged(A0,A1)) 
    <b>MPI_Win_post</b>(neighbors, (<b>MPI_MODE_NOCHECK</b> | <b>MPI_MODE_NOPUT</b>), win0); 
  <b>MPI_Win_complete</b>(win1); 
  <b>MPI_Win_wait</b>(win1); 
} 
</tt></pre> 
  
<P> 
An <font face="sans-serif"> MPI</font> process posts the local window associated with  
<tt>win0</tt> before it completes <font face="sans-serif"> RMA</font> accesses to  
the remote windows associated with <tt>win1</tt>.  
When the call to <font face="sans-serif"> MPI_WIN_WAIT</font> on <tt>win1</tt>  
returns, then all neighbors of the calling <font face="sans-serif"> MPI</font> process have posted the  
windows associated with <tt>win0</tt>. Conversely, when the   
call to <font face="sans-serif"> MPI_WIN_WAIT</font> on <tt>win0</tt> returns, then all neighbors of the calling <font face="sans-serif"> MPI</font> process  
have posted the windows associated with <tt>win1</tt>.  
Therefore, the <font face="sans-serif"> MPI_MODE_NOCHECK</font> option can be used with the calls to  
<font face="sans-serif"> MPI_WIN_START</font>.  
<P> 
Put operations can be used, instead of get operations, if the area of array  
<tt>A0</tt> (resp. <tt>A1</tt>) used by <tt>update(A1, A0)</tt>  
(resp. <tt>update(A0, A1)</tt>) is disjoint from the area  
modified by the <font face="sans-serif"> RMA</font> operation.  On some systems, a put operation may be  
more efficient than a get operation, as it requires information exchange  
only in one direction.  
<P> 
  
<P> 
In the next several examples, for conciseness, the expression  
  
<P><img width=330 height=17 src="img54.gif" alt="Image file"><P>
  
means to perform a get-accumulate operation with the result  
buffer (given by <font face="sans-serif"> result_addr</font> in the description of  
<font face="sans-serif"> MPI_GET_ACCUMULATE</font>) on the left side of the  
assignment, in   
this case, <font face="sans-serif"> z</font>.  This format is also used with  
<font face="sans-serif"> MPI_COMPARE_AND_SWAP</font> and <font face="sans-serif"> MPI_COMM_SIZE</font>.  
Process B<i>...</i> refers to any process other than A.  
<P> 
<br><b> Example</b>  
The following example implements a naive, nonscalable counting  
semaphore.  The example demonstrates the use of  
<font face="sans-serif"> MPI_WIN_SYNC</font> to manipulate the public copy of <tt>X</tt>, as well  
as <font face="sans-serif"> MPI_WIN_FLUSH</font> to complete operations without closing the  
access epoch opened with <font face="sans-serif"> MPI_WIN_LOCK_ALL</font>.  To avoid the  
rules regarding synchronization of the public and private copies of  
windows, <font face="sans-serif"> MPI_ACCUMULATE</font> and <font face="sans-serif"> MPI_GET_ACCUMULATE</font>  
are used to write to or read from the local public copy.  
  
<P><img width=893 height=416 src="img55.gif" alt="Image file"><P>
  
  
<P> 
<br><b> Example</b>  
[Critical region with <font face="sans-serif"> RMA</font>]NeutralCritical region with RMA@Critical region with <font face="sans-serif"> RMA</font>MPI_Barrier,MPI_Accumulate,MPI_Win_sync,MPI_Get_accumulate,MPI_Win_flush,MPI_Win_flush_allImplementing a critical region between two <font face="sans-serif"> MPI</font> processes (Peterson's  
algorithm).  Despite their appearance in the  
following example, <font face="sans-serif"> MPI_WIN_LOCK_ALL</font> and  
<font face="sans-serif"> MPI_WIN_UNLOCK_ALL</font> are not collective calls, but it is  
frequently useful to open shared access epochs to all <font face="sans-serif"> MPI</font> processes from  
all other <font face="sans-serif"> MPI</font> processes in a window.  Once the access epochs are  
opened, accumulate operations as well as flush and sync  
synchronization can be used to read from or write to the  
public copy of the window.  
  
<P><img width=880 height=566 src="img56.gif" alt="Image file"><P>
  
  
<P> 
<br><b> Example</b>  
Implementing a critical region between multiple <font face="sans-serif"> MPI</font> processes with compare  
and swap.  The call to <font face="sans-serif"> MPI_WIN_SYNC</font> is necessary on  
Process A after local initialization of <tt>A</tt> to guarantee the public copy  
has been updated with the initialization value found in the private  
copy.  It would also be valid to call <font face="sans-serif"> MPI_ACCUMULATE</font> with  
<font face="sans-serif"> MPI_REPLACE</font> to directly initialize the public copy.  A call  
to <font face="sans-serif"> MPI_WIN_FLUSH</font> would be necessary to assure <tt>A</tt> in the  
public copy of Process A had been updated before the barrier.  
  
<P><img width=905 height=342 src="img57.gif" alt="Image file"><P>
  
  
<P> 
<br><b> Example</b>The following example demonstrates the proper synchronization in the  
unified memory model when a data transfer is implemented with load and  
store accesses in the case of windows in <em> shared memory</em> (instead of using <font face="sans-serif"> MPI_PUT</font> or  
<font face="sans-serif"> MPI_GET</font>) and the synchronization between <font face="sans-serif"> MPI</font> processes is performed using  
point-to-point communication. The synchronization between <font face="sans-serif"> MPI</font> processes  
must be supplemented with a memory synchronization through calls to  
<font face="sans-serif"> MPI_WIN_SYNC</font>, which act locally as a processor-memory barrier. In  
Fortran, if <font face="sans-serif"> MPI_ASYNC_PROTECTS_NONBLOCKING</font> is  
<tt>.FALSE.</tt>  
or the variable <tt>X</tt> is not declared as <tt> ASYNCHRONOUS</tt>,  
reordering of the accesses to the  
variable <tt>X</tt> must be prevented with <font face="sans-serif"> MPI_F_SYNC_REG</font>  
operations. (No equivalent function is needed in C.)  
<P> 
The variable <tt>X</tt> is contained within a <em> shared memory window</em> and <tt>X</tt>  
corresponds to the same memory location at both processes. The first call to  
<font face="sans-serif"> MPI_WIN_SYNC</font> performed by process A ensures completion of  
the load/store accesses issued by process A. The first call to <font face="sans-serif"> MPI_WIN_SYNC</font>  
performed by process B ensures that process A's updates to <tt>X</tt>  
are visible to process B.  
Similarly, the second call to <font face="sans-serif"> MPI_WIN_SYNC</font> on each process ensures  
correct ordering of the point-to-point communication and thus that the load/store  
operations on process B have completed before any subsequent  
load/store accesses to the variable <tt>X</tt> in process A.  
<P> 
  
<P><img width=744 height=567 src="img58.gif" alt="Image file"><P>
  
  
<P> 
<br><b> Example</b>  
The following example shows how request-based operations can be used  
to overlap communication with computation.  Each <font face="sans-serif"> MPI</font> process fetches,  
processes, and writes the result for <tt>NSTEPS</tt> chunks of data.  Instead  
of a single buffer, <tt>M</tt> local buffers are used to allow up to <tt>M</tt>  
communication operations to overlap with computation.   
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt><b>int</b>         i, j; 
<b>MPI_Win</b>     win; 
<b>MPI_Request</b> put_req[M] = { <b>MPI_REQUEST_NULL</b> }; 
<b>MPI_Request</b> get_req; 
<b>double</b>      *baseptr; 
<b>double</b>      data[M][N]; 
 
<b>MPI_Win_allocate</b>(NSTEPS*N*<b>sizeof</b>(<b>double</b>), <b>sizeof</b>(<b>double</b>), <b>MPI_INFO_NULL</b>, 
                 <b>MPI_COMM_WORLD</b>, &amp;baseptr, &amp;win); 
 
<b>MPI_Win_lock_all</b>(0, win); 
 
<b>for</b> (i = 0; i &lt; NSTEPS; i++) { 
 <b>if</b> (i&lt;M) 
   j=i; 
 <b>else</b> 
   <b>MPI_Waitany</b>(M, put_req, &amp;j, <b>MPI_STATUS_IGNORE</b>); 
 
 <b>MPI_Rget</b>(data[j], N, <b>MPI_DOUBLE</b>, target, i*N, N, <b>MPI_DOUBLE</b>, win, 
          &amp;get_req); 
 <b>MPI_Wait</b>(&amp;get_req,<b>MPI_STATUS_IGNORE</b>); 
 compute(i, data[j], ...); 
 <b>MPI_Rput</b>(data[j], N, <b>MPI_DOUBLE</b>, target, i*N, N, <b>MPI_DOUBLE</b>, win, 
          &amp;put_req[j]); 
} 
 
<b>MPI_Waitall</b>(M, put_req, <b>MPI_STATUSES_IGNORE</b>); 
<b>MPI_Win_unlock_all</b>(win); 
</tt></pre> 
  
  
<P> 
<br><b> Example</b>  
The following example constructs a distributed shared linked list using dynamic  
windows.  Initially process 0 creates the head of the list, attaches it to  
the window, and broadcasts the pointer to all <font face="sans-serif"> MPI</font> processes.  All <font face="sans-serif"> MPI</font> processes then  
concurrently append <tt>N</tt> new elements to the list.  When an <font face="sans-serif"> MPI</font>  
process attempts to   
attach its element to the tail of the list it may discover that its tail pointer  
is stale and it must chase ahead to the new tail before the element can be  
attached.  
This example requires some modification to  
work in an environment where the layout of the structures is different on  
different <font face="sans-serif"> MPI</font> processes.  
<P> 
<br> 
<pre style="background-color:#EFEFEF"><tt>... 
<b>#define</b> NUM_ELEMS 10 
 
<b>#define</b> LLIST_ELEM_NEXT_RANK ( offsetof(llist_elem_t, next) + \ 
                               offsetof(llist_ptr_t, rank) ) 
<b>#define</b> LLIST_ELEM_NEXT_DISP ( offsetof(llist_elem_t, next) + \ 
                               offsetof(llist_ptr_t, disp) ) 
 
/* Linked list pointer */ 
<b>typedef</b> <b>struct</b> { 
  <b>MPI_Aint</b> disp; 
  <b>int</b>      rank; 
} llist_ptr_t; 
 
/* Linked list element */ 
<b>typedef</b> <b>struct</b> { 
  llist_ptr_t next; 
  <b>int</b> value; 
} llist_elem_t; 
 
const llist_ptr_t nil = { (<b>MPI_Aint</b>) <b>MPI_BOTTOM</b>, -1 }; 
 
/* List of locally allocated list elements. */ 
static llist_elem_t **my_elems = NULL; 
static <b>int</b> my_elems_size  = 0; 
static <b>int</b> my_elems_count = 0; 
 
/* Allocate a new shared linked list element */ 
<b>MPI_Aint</b> alloc_elem(<b>int</b> value, <b>MPI_Win</b> win) { 
  <b>MPI_Aint</b> disp; 
  llist_elem_t *elem_ptr; 
 
  /* Allocate the new element and <b>register</b> it with the window */ 
  <b>MPI_Alloc_mem</b>(<b>sizeof</b>(llist_elem_t), <b>MPI_INFO_NULL</b>, &amp;elem_ptr); 
  elem_ptr-&gt;value = value; 
  elem_ptr-&gt;next  = nil; 
  <b>MPI_Win_attach</b>(win, elem_ptr, <b>sizeof</b>(llist_elem_t)); 
 
  /* Add the element to the list of local elements so we can free 
     it later. */ 
  <b>if</b> (my_elems_size == my_elems_count) { 
    my_elems_size += 100; 
    my_elems = realloc(my_elems, my_elems_size*<b>sizeof</b>(<b>void</b>*)); 
  } 
  my_elems[my_elems_count] = elem_ptr; 
  my_elems_count++; 
 
  <b>MPI_Get_address</b>(elem_ptr, &amp;disp); 
  <b>return</b> disp; 
} 
 
<b>int</b> main(<b>int</b> argc, <b>char</b> *argv[]) { 
  <b>int</b>           procid, nproc, i; 
  <b>MPI_Win</b>       llist_win; 
  llist_ptr_t   head_ptr, tail_ptr; 
 
  <b>MPI_Init</b>(&amp;argc, &amp;argv); 
 
  <b>MPI_Comm_rank</b>(<b>MPI_COMM_WORLD</b>, &amp;procid); 
  <b>MPI_Comm_size</b>(<b>MPI_COMM_WORLD</b>, &amp;nproc); 
 
  <b>MPI_Win_create_dynamic</b>(<b>MPI_INFO_NULL</b>, <b>MPI_COMM_WORLD</b>, &amp;llist_win); 
 
  /* Process 0 creates the head node */ 
  <b>if</b> (procid == 0) 
    head_ptr.disp = alloc_elem(-1, llist_win); 
 
  /* Broadcast the head pointer to everyone */ 
  head_ptr.rank = 0; 
  <b>MPI_Bcast</b>(&amp;head_ptr.disp, 1, <b>MPI_AINT</b>, 0, <b>MPI_COMM_WORLD</b>); 
  tail_ptr = head_ptr; 
 
  /* Lock the window <b>for</b> shared access to all targets */ 
  <b>MPI_Win_lock_all</b>(0, llist_win); 
 
  /* All processes concurrently append NUM_ELEMS elements to the list */ 
  <b>for</b> (i = 0; i &lt; NUM_ELEMS; i++) { 
    llist_ptr_t new_elem_ptr; 
    <b>int</b> success; 
 
    /* Create a new list element and attach it to the window */ 
    new_elem_ptr.rank = procid; 
    new_elem_ptr.disp = alloc_elem(procid, llist_win); 
 
    /* Append the new node to the list.  This might take multiple  
       attempts <b>if</b> others have already appended and our tail pointer  
       is stale. */ 
    <b>do</b> { 
      llist_ptr_t next_tail_ptr = nil; 
 
      <b>MPI_Compare_and_swap</b>((<b>void</b>*) &amp;new_elem_ptr.rank, (<b>void</b>*) &amp;nil.rank, 
          (<b>void</b>*)&amp;next_tail_ptr.rank, <b>MPI_INT</b>, tail_ptr.rank, 
          <b>MPI_Aint_add</b>(tail_ptr.disp, LLIST_ELEM_NEXT_RANK), 
          llist_win); 
 
      <b>MPI_Win_flush</b>(tail_ptr.rank, llist_win); 
      success = (next_tail_ptr.rank == nil.rank); 
 
      <b>if</b> (success) { 
        <b>MPI_Accumulate</b>(&amp;new_elem_ptr.disp, 1, <b>MPI_AINT</b>, tail_ptr.rank, 
            <b>MPI_Aint_add</b>(tail_ptr.disp, LLIST_ELEM_NEXT_DISP), 1, 
            <b>MPI_AINT</b>, <b>MPI_REPLACE</b>, llist_win); 
 
        <b>MPI_Win_flush</b>(tail_ptr.rank, llist_win); 
        tail_ptr = new_elem_ptr; 
 
      } <b>else</b> { 
        /* Tail pointer is stale, fetch the displacement.  May take 
           multiple tries <b>if</b> it is being updated. */ 
        <b>do</b> { 
          <b>MPI_Get_accumulate</b>(NULL, 0, <b>MPI_AINT</b>, &amp;next_tail_ptr.disp, 
              1, <b>MPI_AINT</b>, tail_ptr.rank, 
              <b>MPI_Aint_add</b>(tail_ptr.disp, LLIST_ELEM_NEXT_DISP), 
              1, <b>MPI_AINT</b>, <b>MPI_NO_OP</b>, llist_win); 
 
          <b>MPI_Win_flush</b>(tail_ptr.rank, llist_win); 
        } <b>while</b> (next_tail_ptr.disp == nil.disp); 
        tail_ptr = next_tail_ptr; 
      } 
    } <b>while</b> (!success); 
  } 
 
  <b>MPI_Win_unlock_all</b>(llist_win); 
  <b>MPI_Barrier</b>(<b>MPI_COMM_WORLD</b>); 
 
  /* Free all the elements in the list */ 
  <b>for</b> ( ; my_elems_count &gt; 0; my_elems_count--) { 
    <b>MPI_Win_detach</b>(llist_win,my_elems[my_elems_count-1]); 
    <b>MPI_Free_mem</b>(my_elems[my_elems_count-1]); 
  } 
  <b>MPI_Win_free</b>(&amp;llist_win); 
... 
</tt></pre> 
  
  
<P> 
<P> 
<P> 
  
  

<P>
<hr>
<a href="node343.htm#Node343"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node308.htm#Node308"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node345.htm#Node345"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node308.htm#Node308"> One-Sided Communications</a>
<b>Next: </b><a href="node345.htm#Node345"> External Interfaces</a>
<b>Previous: </b><a href="node343.htm#Node343"> Registers and Compiler Optimizations</a>
<p>
<HR>
Return to <A HREF="node627.htm">MPI-5.0 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-5.0 of June 9, 2025<BR>
HTML Generated on March 2, 2025
</FONT>
</body>
</html>
